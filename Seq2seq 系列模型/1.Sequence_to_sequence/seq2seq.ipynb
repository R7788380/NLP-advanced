{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"seq2seq.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"id":"eisd_CL4-maw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148330293,"user_tz":-480,"elapsed":957,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"5ee2b222-7b6d-4957-b5e8-04a578c19dbe"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Nov 12 02:32:13 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FLc5D2cU-pp7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148360239,"user_tz":-480,"elapsed":28090,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"fee2cf32-7157-4e92-bc06-c83566622457"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jcT7MIxnGTJC"},"source":["* @file NLP進階 / Seq2seq\n","  * @brief Seq2seq 模型實作 \n","\n","  * 此份程式碼是以教學為目的，附有完整的架構解說。\n","\n","  * @author 人工智慧科技基金會 AI 工程師 - 康文瑋\n","  * Email: run963741@aif.tw\n","  * Resume: https://www.cakeresume.com/run963741\n","\n","  * 最後更新日期: 2020/11/13"]},{"cell_type":"markdown","metadata":{"id":"nAiIhvyLu8d3"},"source":["# Sequence to sequence (Seq2seq)\n","\n","[Sequence to sequence](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) (序列到序列) 模型是在 2014 年由 Google 團隊的 Sutskever 等人所提出，這個架構首次將類神經網路應用在機器翻譯任務上，造成了不小的轟動，此架構成為往後自然語言處理領域的各種模型的基礎架構，許多知名的模型都是由此架構下去作延伸修改。\n","\n","Seq2seq 常見的應用場景例如：\n","* 機器翻譯 (Machine Translation)：語言之間的翻譯，通常兩語言的序列長度通常會不一樣。\n","* 文本摘要 (Text Summarization)：從文本中萃取最重要的摘要，兩者的序列長度一定不一樣。\n","* 聊天機器人 (Chatbot): 聊天一來一往的句子長度通常會不一樣。"]},{"cell_type":"markdown","metadata":{"id":"eZotRz1uu6xJ"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=19OpGD0XFy1W-IOmBaPOMfv195vU-rr4O' width=\"800\"/>\n","<figcaption></figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"rkdslBbP6Wuh"},"source":["# Environment\n","\n","#### - Tensorflow 2.3.0\n","#### - python3.7"]},{"cell_type":"markdown","metadata":{"id":"T0RAfBBMy1tT"},"source":["# 載入套件"]},{"cell_type":"code","metadata":{"id":"JjJJyJTZYebt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148366787,"user_tz":-480,"elapsed":6527,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"9efcfe90-73cf-45cc-a1b5-afea10bce564"},"source":["import tensorflow_datasets as tfds\n","import tensorflow_addons as tfa\n","import tensorflow as tf\n","import os\n","import tqdm\n","import unicodedata\n","import re\n","import io\n","from pprint import pprint\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","os.chdir('/content/drive/Shared drives/類技術班教材/標準版/NLP進階/Seq2seq 系列模型/Sequence_to_sequence')\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fd1NWMxjfsDd"},"source":["# 建立資料夾路徑\n","\n","`en_vocab_file`: 儲存英文字典 (vocabulary) 路徑\n","\n","`sp_vocab_file`: 儲存西文字典 (vocabulary) 路徑\n","\n","`checkpoint_path`: 儲存模型路徑\n","\n","`download_dir`: 資料儲存路徑"]},{"cell_type":"code","metadata":{"id":"4wKndea96Wum"},"source":["output_dir = \"nmt_seq2seq\"\n","en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n","sp_vocab_file = os.path.join(output_dir, \"sp_vocab\")\n","download_dir = \"tensorflow-datasets/downloads\"\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","if not os.path.exists(download_dir):\n","    os.makedirs(download_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntjxFlUo6Wuv"},"source":["# 下載 `Tensorflow` 範例資料集\n","\n","下載英文和西班牙文的範例資料集。"]},{"cell_type":"code","metadata":{"id":"4IP6dH3d79pP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148969032,"user_tz":-480,"elapsed":4925,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"c41bc084-3aab-4bd7-b5c5-da339ee53739"},"source":["# Download the file\n","path_to_zip = tf.keras.utils.get_file(\n","    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True, cache_dir = download_dir)\n","\n","path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n","\n","print('--'*20)\n","print('資料路徑: ', path_to_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","資料路徑:  tensorflow-datasets/downloads/datasets/spa-eng/spa.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zbs5IG7kaiTM"},"source":["# 資料前處理"]},{"cell_type":"markdown","metadata":{"id":"vbo607Xdazgp"},"source":["## 字串處理\n","\n","在 Unicode 中，某些字符能夠用多種合法的底層編碼，例如在以下範例中，西班牙文字 $\\tilde{n}$ 可以由兩種編碼來表示，這種情況會導致後續建立語言模型時產生問題，所以我們要使用 `unicodedata` 將文字做標準化。"]},{"cell_type":"code","metadata":{"id":"1-PQrj8xbKVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148438752,"user_tz":-480,"elapsed":1416,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"c835831b-587a-4ab2-8631-91888eac1f8e"},"source":["s1 = 'Spicy Jalape\\u00f1o'\n","s2 = 'Spicy Jalapen\\u0303o'\n","\n","print('s1: ', s1)\n","print('s2: ', s2)\n","print('s1 和 s2 是否一樣: ', s1 == s2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["s1:  Spicy Jalapeño\n","s2:  Spicy Jalapeño\n","s1 和 s2 是否一樣:  False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3vkiu10Exl1o"},"source":["### `unicodedata` 標準化範例"]},{"cell_type":"code","metadata":{"id":"dT605N4sxp90","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148695568,"user_tz":-480,"elapsed":970,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"00e25abf-d1cb-498e-e2b1-b8058c843ecb"},"source":["# https://python3-cookbook.readthedocs.io/zh_CN/latest/c02/p09_normalize_unicode_text_to_regexp.html\n","s1_normalized = unicodedata.normalize('NFD', s1)\n","s2_normalized = unicodedata.normalize('NFD', s2)\n","\n","print('s1: ', s1_normalized)\n","print('s2: ', s2_normalized)\n","print('s1 和 s2 是否一樣: ', s1_normalized == s2_normalized)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["s1:  Spicy Jalapeño\n","s2:  Spicy Jalapeño\n","s1 和 s2 是否一樣:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ai75FUKW1Mq2"},"source":["## 字串處理函數"]},{"cell_type":"code","metadata":{"id":"PedIwQ2r8ZIl"},"source":["def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn') # Mn 判斷是否為 Nonspacing\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","\n","  # 正則表達式: https://www.runoob.com/regexp/regexp-syntax\n","  # http://ccckmit.wikidot.com/regularexpression\n","  w = re.sub(r\"([?.!,¿])\", r\"\\1\", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","  w = w.strip()\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0p8-lyHI8P20"},"source":["def create_dataset(path, num_examples):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","  \n","  # 使用 /t 把英文和西文分開\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","\n","  return zip(*word_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpGYs76O1ix4"},"source":["## 讀取資料集"]},{"cell_type":"code","metadata":{"id":"PU8kyLrm8mp6"},"source":["en, sp = create_dataset(path_to_file, None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnOaS2SrC7MR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605148978660,"user_tz":-480,"elapsed":960,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"30da1337-aa15-43ec-bf04-7bc7f5b7ec57"},"source":["print(en[20])\n","print(sp[20])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["wait.\n","esperen.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k8IJqAQqKUgG"},"source":["## 切割訓練集 (Training) 和測試集 (Testing)"]},{"cell_type":"code","metadata":{"id":"gk2OVBfrKag4"},"source":["en_train, en_test, sp_train, sp_test = train_test_split(en, sp, test_size = 0.1, shuffle = True)\n","\n","train_examples = tf.data.Dataset.from_tensor_slices((en_train, sp_train))\n","test_examples = tf.data.Dataset.from_tensor_slices((en_test, sp_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e_xB4ekQOZZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605149008885,"user_tz":-480,"elapsed":7189,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"73f7e445-50f6-4b1c-d806-1edece4b4389"},"source":["print('Train size: ', len(en_train))\n","print('Test size: ', len(en_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train size:  107067\n","Test size:  11897\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"03Kr0yj1LBST","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605149108863,"user_tz":-480,"elapsed":1006,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"175e2b73-2ebd-425d-ee32-c686038d3f42"},"source":["# http://ez2learn.com/basic/unicode.html\n","tmp_en, tmp_sp = next(iter(train_examples))\n","\n","print('Input english: ', tmp_en)\n","print('Output spanish: ', tmp_sp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input english:  tf.Tensor(b'he sells cars.', shape=(), dtype=string)\n","Output spanish:  tf.Tensor(b'el vende carros.', shape=(), dtype=string)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y9e7Njms6Wu-"},"source":["## 使用`tfds.deprecated.text.SubwordTextEncoder`載入與建立字典\n","\n","* `.load_from_file`: 從路徑載入字典\n","* `.build_from_corpus`: 建立字典\n","* `.save_to_file`: 儲存字典"]},{"cell_type":"code","metadata":{"id":"8qYu_H9N6WvD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605153036500,"user_tz":-480,"elapsed":1525,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"e11473d2-8aad-4dd6-c3a5-2754660bacfb"},"source":["%%time\n","try:\n","    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file) \n","    print('Load English vocabulary: %s' % en_vocab_file)\n","except:\n","    print('Build English vocabulary: %s' % en_vocab_file)\n","    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() for en, sp in train_examples),target_vocab_size = 2**13)\n","    tokenizer_en.save_to_file(en_vocab_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load English vocabulary: nmt_seq2seq/en_vocab\n","CPU times: user 32.5 ms, sys: 6.06 ms, total: 38.5 ms\n","Wall time: 629 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KVBg5Q8tBk5z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605153037846,"user_tz":-480,"elapsed":2704,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"7f8d007f-b903-4c8b-cc9f-8e1a0505a593"},"source":["%%time\n","try: \n","    tokenizer_sp = tfds.deprecated.text.SubwordTextEncoder.load_from_file(sp_vocab_file) \n","    print('Load Spanish vocfabulary: %s' % sp_vocab_file)\n","except: \n","    print('Build Spanish vocabulary: %s' % sp_vocab_file)\n","    tokenizer_sp = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((sp.numpy() for en, sp in train_examples), target_vocab_size = 2**13)\n","    tokenizer_sp.save_to_file(sp_vocab_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load Spanish vocfabulary: nmt_seq2seq/sp_vocab\n","CPU times: user 32.1 ms, sys: 4.72 ms, total: 36.8 ms\n","Wall time: 613 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rHDcuv8K6M79"},"source":["### 字典大小以及 `subword`"]},{"cell_type":"code","metadata":{"id":"1-xZvaYj6WvF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605153039377,"user_tz":-480,"elapsed":1206,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"3902c702-75b1-49e5-c077-4449de831567"},"source":["print('English vocabulary size: ', tokenizer_en.vocab_size)\n","print('Spanish vocabulary size: ', tokenizer_sp.vocab_size)\n","print('-' * 30)\n","print('English subwords: ', tokenizer_en.subwords[:10])\n","print('Spanish subwords: ', tokenizer_sp.subwords[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["English vocabulary size:  8260\n","Spanish vocabulary size:  8078\n","------------------------------\n","English subwords:  ['i_', 'the_', 'to_', 'you_', 'tom_', 'a_', 't_', 'is_', 'he_', 's_']\n","Spanish subwords:  ['que_', 'de_', 'el_', 'a_', 'no_', 'la_', 'tom_', '¿', 'en_', 'es_']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sEqjnOoz6WvH"},"source":["### Example\n","\n","英文的斷詞方式是以 [`wordpiece`](https://arxiv.org/pdf/1609.08144.pdf) 進行斷詞。"]},{"cell_type":"code","metadata":{"id":"4DYWukNFkGQN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605154040681,"user_tz":-480,"elapsed":1036,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"aad0a605-bc61-4e93-8756-ba3ed268e698"},"source":["sample_string = 'Transformer is awesome.'\n","\n","tokenized_string_token = tokenizer_en.encode(sample_string)\n","print ('Tokenized string token is \\n{}'.format(tokenized_string_token))\n","\n","print('-'*20)\n","tokenized_string = [tokenizer_en.decode([ts]) for ts in tokenized_string_token]\n","print('Tokenized srting is \\n{}'.format(tokenized_string))\n","\n","print('-'*20)\n","original_string = tokenizer_en.decode(tokenized_string_token)\n","print ('The original string: \\n{}'.format(original_string))\n","\n","assert original_string == sample_string"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tokenized string token is \n","[8088, 2692, 8119, 6761, 8, 4368, 8050]\n","--------------------\n","Tokenized srting is \n","['T', 'ran', 's', 'former ', 'is ', 'awesome', '.']\n","--------------------\n","The original string: \n","Transformer is awesome.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7cvjtgiO6WvL"},"source":["## 添加`<BOS>`,`<EOS>`在句子頭尾\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=11_d2u6W8t3qi_T8x22jKQ-MYnBNM6-JW' width=\"500\"/>\n","<figcaption>Auto Regressive</figcaption></center>\n","</figure>\n","\n","Seq2seq 模型的訓練方式以及預測方式都是使用 Auto Regressive 的模式來進行，將當前時間點 $T_1$ 的預測值接在下一個時間點 $T_2$ 的後面，再輸入給模型，直到模型預測出 $<EOS>$ 為止。\n","\n","* $<BOS>$: 全名為 Begin of sentence，因為第一個時間點 $T_1$ 翻譯時不可能馬上有正確答案，所以會統一輸入 $<BOS>$ ，例如上圖的要完整預測出 $文瑋助教真帥$，第一個時間點 $T_1$ 還沒有 $文$ 這個字，所以使用 $<BOS>$ 來作為輸入。\n","\n","* $<EOS>$: 全名為 End of sentence，當模型預測出這個 token 時，就代表整個序列預測完畢，如果前處理沒有加上 $<EOS>$，模型就會永無止盡的預測下去。"]},{"cell_type":"code","metadata":{"id":"UZwnPr4R055s"},"source":["def encode(en_t, sp_t):\n","    \"\"\"\n","    這邊將 `.vocab_size`視為`<BOS>`, `.vocab_size+1`視為`<EOS>`\n","    訓練集所有句子都需要進行這一步前處理\n","    \"\"\"\n","    en_indics = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy()) + [tokenizer_en.vocab_size + 1]\n","    sp_indics = [tokenizer_sp.vocab_size] + tokenizer_sp.encode(sp_t.numpy()) + [tokenizer_sp.vocab_size + 1]\n","\n","    return en_indics, sp_indics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ug-KgwUH6WvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605154177378,"user_tz":-480,"elapsed":1115,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"9e390c9b-cfac-42ef-9a0c-0ed477dcdc37"},"source":["en_t, sp_t = next(iter(train_examples))\n","en_indics, sp_indics = encode(en_t, sp_t)\n","\n","print('英文<BOS>: %d' % tokenizer_en.vocab_size)\n","print('英文<EOS>: %d' % (tokenizer_en.vocab_size + 1))\n","print('西文<BOS>: %d' % tokenizer_sp.vocab_size)\n","print('西文<EOS>: %d' % (tokenizer_sp.vocab_size + 1))\n","\n","print('-' * 20)\n","print('Before encode: (two tensor):')\n","print(en_t)\n","print(sp_t)\n","print()\n","print('After encode: (two array): ')\n","print(en_indics)\n","print(sp_indics)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["英文<BOS>: 8260\n","英文<EOS>: 8261\n","西文<BOS>: 8078\n","西文<EOS>: 8079\n","--------------------\n","Before encode: (two tensor):\n","tf.Tensor(b'he sells cars.', shape=(), dtype=string)\n","tf.Tensor(b'el vende carros.', shape=(), dtype=string)\n","\n","After encode: (two array): \n","[8260, 9, 2684, 1892, 8050, 8261]\n","[8078, 3, 3470, 1836, 7937, 7868, 8079]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tx1sFbR-9fRs"},"source":["### `tf.py_function`\n","\n","在 Tensorflow 的訓練過程中，所有的計算過程都必須使用 `tf.` 來達成，當有某一些函數操作不涉及到 `tf.` 時，就必須使用 `tf.py_function` 來將函數納入 tensorflow 的計算流程裡面。"]},{"cell_type":"code","metadata":{"id":"By6N4r_16WvR"},"source":["# import traceback\n","\n","# try:\n","#     train_examples.map(encode)\n","# except AttributeError:\n","#     traceback.print_exc()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mah1cS-P70Iz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604925000778,"user_tz":-480,"elapsed":1304,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"3c4e15ba-9231-4687-96c0-0115b4c7d761"},"source":["def tf_encode(en_t, sp_t):\n","    \"\"\"\n","    使用 tf.py_function 將 encode 函數轉換為 tensorflow 的輸入與輸出\n","    \"\"\"\n","    return tf.py_function(encode, [en_t, sp_t], [tf.int64, tf.int64])\n","\n","tmp_dataset = train_examples.map(tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","en_indices, sp_indices = next(iter(tmp_dataset))\n","\n","print('After tf_encode: (two tensor)')\n","print(en_indices)\n","print(sp_indices)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["After tf_encode: (two tensor)\n","tf.Tensor([8260    2  693  245 1525    3    2 1820 8050 8261], shape=(10,), dtype=int64)\n","tf.Tensor([8078  230    6  425  383    2  456   24    6 2001 7868 8079], shape=(12,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ds3x62Sv6WvY"},"source":["## 限制句子長度\n","\n","為了加快訓練速度，使用`tf.logical`限制句子長度，並使用`.filter`過濾"]},{"cell_type":"code","metadata":{"id":"c081xPGv1CPI"},"source":["max_length = 50\n","def filter_max_length(en_t, sp_t, max_length = max_length):\n","    \n","    return tf.logical_and(tf.size(en_t) <= max_length,\n","                          tf.size(sp_t) <= max_length)\n","\n","tmp_dataset = tmp_dataset.filter(filter_max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9i3ayTB6Wvb"},"source":["## Padding\n","\n","指定 `batch_size` 以及 `padding`，`padding` 會先檢查一個 `batch` 裡面的句子長度，不足最大長度的句子會補 `0` (`padding index`)，因為預設是補 `0` 的關係，所以字典中的  `word index` 必須從 `1` 開始計算，不然會跟 `padding index` 混淆。"]},{"cell_type":"code","metadata":{"id":"GtU484lD6Wvc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604925003084,"user_tz":-480,"elapsed":2187,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"0b6b51c3-a9b2-4cad-f451-7720a4501edd"},"source":["batch_size = 64\n","tmp_dataset = tmp_dataset.padded_batch(batch_size=batch_size, padded_shapes=([-1], [-1]))\n","\n","en_batch, sp_batch = next(iter(tmp_dataset))\n","\n","print('英文 batch: ')\n","print(en_batch)\n","print('-' * 15)\n","print('西文 batch: ')\n","print(sp_batch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["英文 batch: \n","tf.Tensor(\n","[[8260    2  693 ...    0    0    0]\n"," [8260   27    4 ...    0    0    0]\n"," [8260   20 6620 ...    0    0    0]\n"," ...\n"," [8260    9 2129 ...    0    0    0]\n"," [8260    1   35 ...    0    0    0]\n"," [8260   58    2 ...    0    0    0]], shape=(64, 20), dtype=int64)\n","---------------\n","西文 batch: \n","tf.Tensor(\n","[[8078  230    6 ...    0    0    0]\n"," [8078    8   63 ...    0    0    0]\n"," [8078   14 5860 ...    0    0    0]\n"," ...\n"," [8078    3 5315 ...    0    0    0]\n"," [8078   45  460 ...    0    0    0]\n"," [8078   64   21 ...    0    0    0]], shape=(64, 20), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D1Ou_LXp6Wve"},"source":["# 將`train_examples`與`val_examples`做同樣處理\n","\n","* `train`:\n","\n"," - `map(tf_encode)`: 將字串轉成index\n"," - `filter(filter_max_length)`:過濾長度\n"," - `cache()`: cache the dataset to memory to get a speedup while reading from it.\n"," - `shuffle(buffer_size)`: 打亂buffer裡的資料，確保隨機\n"," - `padded_batch(batch_size, padded_shapes=([-1],[-1]))`: padding長度 \n","\n","Tensor-core pipeline: https://www.tensorflow.org/guide/performance/datasets?hl=zh_cn"]},{"cell_type":"code","metadata":{"id":"9mk9AZdZ5bcS"},"source":["max_length = 50\n","batch_size = 64\n","buffer_size = 15000\n","\n","train_dataset = (train_examples\n","                 .map(tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                 .filter(filter_max_length)\n","                 .cache()\n","                 .shuffle(buffer_size)\n","                 .padded_batch(batch_size, padded_shapes=([-1],[-1]))\n","                 .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","test_dataset = (test_examples\n","               .map(tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","               .filter(filter_max_length)\n","               .padded_batch(batch_size, padded_shapes=([-1], [-1])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fXvfYVfQr2n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604925009788,"user_tz":-480,"elapsed":8504,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"0521a60a-604b-466f-d4da-f967d604c53e"},"source":["%%time\n","en_batch, sp_batch = next(iter(train_dataset))\n","\n","print('英文 batch tensor: ')\n","print(en_batch)\n","print('-' * 20)\n","print('西文 batch tensor: ')\n","print(sp_batch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["英文 batch tensor: \n","tf.Tensor(\n","[[8260    1   35 ...    0    0    0]\n"," [8260   49   14 ...    0    0    0]\n"," [8260   27    4 ...    0    0    0]\n"," ...\n"," [8260   37   58 ... 1060 8050 8261]\n"," [8260   21    8 ...    0    0    0]\n"," [8260    5  117 ... 8050 8261    0]], shape=(64, 16), dtype=int64)\n","--------------------\n","西文 batch tensor: \n","tf.Tensor(\n","[[8078   45    1 ...    0    0    0]\n"," [8078  968   18 ...    0    0    0]\n"," [8078    8   99 ...    0    0    0]\n"," ...\n"," [8078    5   64 ... 1231 7868 8079]\n"," [8078   25   10 ...    0    0    0]\n"," [8078    7   13 ... 8079    0    0]], shape=(64, 17), dtype=int64)\n","CPU times: user 9.86 s, sys: 1.62 s, total: 11.5 s\n","Wall time: 6.94 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HjYgKA4h6Wvm"},"source":["# Seq2seq"]},{"cell_type":"markdown","metadata":{"id":"Wg6Df3XGPam7"},"source":["架構如下圖，Seq2seq 由編碼器 (Encoder) 以及 解碼器 (Decoder) 所組成，編碼器和解碼器個別都是一個 LSTM，當然也能替換成其他序列模型，例如 RNN、GRU等等 ：\n","\n","假設有一個序列：\n","$$\n","X=\\{x_1,x_2,...,x_n\\}\n","$$\n","\n","* 編碼器 (Encoder): 負責接收輸入序列，序列通過 LSTM 之後，在每一步都會產生一個 hidden state $h_t$ 以及一個 cell memory $c_t$，以 seq2seq 來說，會使用最後一步的 hidden state $h_n$ 來作為解碼器 (Decoder) 的輸入，例如以我們的例子英中翻譯來說，輸入給編碼器的句子就是英文句子，當編碼器處理完英文句子之後，會將最後濃縮的資訊丟給解碼器。\n","$$\n","h_t, c_t=LSTM(x_t,h_{t-1},c_{t-1})\n","$$\n","\n","* 解碼器 (Decoder): 負責接收編碼器的輸入以及輸出，第一個時間點輸入給 LSTM，之後的輸出方式就是 auto regressive，直到模型輸出 $<EOS>$ 為止，這個方式上面介紹過了。\n","$$\n","\\hat{h}_t, \\hat{c}_t=LSTM(y_t,\\hat{h}_{t-1},\\hat{c}_{t-1})\n","$$\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1AMh4ZXSgXlLrkLPtXOaJXHir4Qt5qUfs' width=\"700\"/>\n","<figcaption>Sequence to sequence</figcaption></center>\n","</figure>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"szHKVijr6Wvm"},"source":["## 基本參數設置"]},{"cell_type":"code","metadata":{"id":"UQpvB32N6Wvm"},"source":["vocab_inp_size = tokenizer_en.vocab_size + 2\n","vocab_tar_size = tokenizer_sp.vocab_size + 2\n","\n","embedding_dim = 512\n","units = 1024\n","batch_size = 64\n","epochs = 20\n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VuD6L_sbPmCy"},"source":["以下程式碼分為兩個主要部分:\n","\n","1. `class Encoder()`: 通常使用`RNN`系列的模型，例如`RNN`、`LSTM`、`GRU`等等，頂多再加上雙向的架構，所有的`RNN`模型在每個`timestep`都會產生一個`hidden state`，最後一個`timestep`的`hidden state`會當成`Decoder`的`initial state`，也可以稱為`Context vector`。\n","\n","2. `class Decoder()`: 負責接收`Encoder`的`Context vector`，然後輸入給 `RNN`，然後輸出方式依照`Auto regressive`方式輸出。"]},{"cell_type":"markdown","metadata":{"id":"VfzIpOOP6Wvp"},"source":["## Encoder\n","\n"," - `return_sequences`：是否返回所有的`timestep`的`hidden_state`。\n"," - `return_state`：是否返回最後一個`timestep`的`cell_state`，注意只有`LSTM`有`cell_state`，其餘像`RNN,GRU`是沒有`cell_state`的。"]},{"cell_type":"code","metadata":{"id":"M8qDnamu6Wvq"},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_inp_size, embedding_dim, units, batch_size):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.units = units\n","        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim)\n","        self.lstm = tf.keras.layers.LSTM(units=units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","        \n","    def __call__(self, x):\n","        \"\"\"\n","        LSTM需要initial_state與initial_cell兩種初始值，這邊都使用同一個initial_hidden_state作為初始值\n","        若使用GRU則只需要一個initial_state即可\n","        \"\"\"\n","        x = self.embedding(x)\n","        states, last_state, cell_memory = self.lstm(x)\n","        return states, last_state\n","\n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_size, self.units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b2rw6iz6Wvt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604925957243,"user_tz":-480,"elapsed":948,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"cc4e8679-ffec-43e6-8b9d-b6836a1cd98c"},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n","\n","initial_hidden = encoder.initialize_hidden_state()\n","states, last_state = encoder(en_batch)\n","print('Encoder hidden state for each timestep: ',states.shape) # (batch_size, sequence_length, units)\n","print('Encoder last hidden state: ',last_state.shape) # (batch_size, sequence_length, units)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoder hidden state for each timestep:  (64, 16, 1024)\n","Encoder last hidden state:  (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gV0nfSyR6Wv5"},"source":["## Decoder\n","\n","透過`Encoder`得到`context vector`之後，`Decoder`的作用就是決定如何串接`Encoder`的`hidden state`以及進行輸出預測，這邊直接把`Context vector`當成`initial state`。"]},{"cell_type":"code","metadata":{"id":"cWwczBd06Wv5"},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_tar_size, embedding_dim, dec_units):\n","    super().__init__()\n","    # 同上式中的 x\n","    self.embedding = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim)\n","    # 同上式中的 lstm\n","    self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    # 同上式中的 Dense\n","    self.fc = tf.keras.layers.Dense(vocab_tar_size)\n","\n","  def __call__(self, x, hidden):\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # passing the concatenated vector to the LSTM\n","    # output shape == (batch_size, 1, dec_units)\n","    # state shape == (batch_size, dec_units)\n","    # cell_memory shape == (batch_size, dec_units)\n","    output, state, cell_memory = self.lstm(x, initial_state = hidden)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.squeeze(output, axis=1)\n","\n","    # output shape == (batch_size, vocab_size)\n","    logits = self.fc(output)\n","\n","    return logits, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct0AL_tf6WwA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604926058609,"user_tz":-480,"elapsed":856,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"24976b1d-ccd7-40de-f125-a48a3ed05208"},"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units)\n","logits, hidden = decoder(sp_batch[:,-1:], [last_state, last_state])\n","\n","print('logits shape: ', logits.shape) # (batch_size, vocab_tar_size)\n","print('last_state shape: ', hidden[0].shape) # (batch_size, dec_units)\n","print('cell memory shape: ', hidden[1].shape) # (batch_size, dec_units)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logits shape:  (64, 8080)\n","last_state shape:  (64, 1024)\n","cell memory shape:  (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rClkkSCc6WwE"},"source":["## Loss and metrics\n","\n","這裏定義損失函數，當 `Decoder` 預測出每個位置的詞時需要計算損失，這邊使用分類任務的損失函數 `CategoricalCrossentropy`。\n","\n","部分句子為因為 `padding` 而有許多的 `0`，但是那並不是真實的標籤，所以必須忽略 `padding` 位置的損失。\n"]},{"cell_type":"code","metadata":{"id":"yy4i6jGQ6WwE"},"source":["def loss_function(real, pred):\n","    \"\"\"\n","    Input:\n","    real: (batch_size, 1)\n","    pred: (batch_size, vocab_tar_size)\n","    \n","    Return: \n","    mean loss for current batch\n","    \n","    mask: (batch_size, 1)\n","    因為一個batch中有些句子會提早出現padding(index為0)，不需要計算 padding 的 loss，這裏使用 mask 來記錄 padding 的 index\n","    \"\"\"\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    \n","    \"\"\"\n","    from_logits: y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution.\n","    reduction: the reduction schedule of output loss vectors. `https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/Reduction`\n","    \"\"\"\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","    \n","    \"\"\"\n","    loss_: (batch_size, 1)\n","    \"\"\"\n","    loss_ = loss_object(real, pred)\n","    \n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask # 只計算非padding的loss\n","    \n","    return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIaHi5DO6WwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604926069755,"user_tz":-480,"elapsed":639,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"80d8fd5d-e86e-49f7-fd44-4e5b70393d0a"},"source":["real = tf.constant([[0.],[1.]], dtype=tf.float32)\n","pred = tf.constant([[0.3,0.2,0.5],[0,1,0]], dtype=tf.float32)\n","\n","mean_loss = loss_function(real, pred)\n","print('mean loss for current batch: ', mean_loss.numpy())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean loss for current batch:  0.42281893\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"coTS7ueS6WwK"},"source":["## Optimizer\n","\n","優化器通常會優先考慮 `Adam`。\n","\n","近期亦有許多表現不錯的優化器被研究出來，例如 `RangerLars` 在圖像任務上表現上就比 `Adam` 還要好。\n","\n","優化器表現可以參考: https://github.com/mgrankin/over9000"]},{"cell_type":"code","metadata":{"id":"EoZDHI-i6WwL"},"source":["# optimizer = tf.keras.optimizers.Adam(learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ec7fASyY9rx6"},"source":["radam = tfa.optimizers.RectifiedAdam()\n","# ranger\n","optimizer = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hj-5-skp6WwM"},"source":["## Checkpoints\n","\n","這裏定義儲存模型的方式:\n","\n","* `checkpoint_path`: 模型儲存路徑\n","* `ckpt`: 模型中的架構\n","* `ckpt_manager`: 模型儲存的策略，包含了架構 (`ckpt`), 路徑 (`checkpoint_path`), 儲存最近幾次 (`max_to_keep`)。"]},{"cell_type":"code","metadata":{"id":"Iy5sFP236WwN"},"source":["model_name = 'checkpoints_seq2seq'\n","\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units)\n","\n","checkpoint_path = os.path.join(output_dir, model_name)\n","ckpt = tf.train.Checkpoint(encoder = encoder, decoder = decoder, optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jPUc-Ky6WwP"},"source":["## Train step\n","\n","這邊設定輸入一個英文句子進行訓練的方式，大致上的步驟為:\n","\n","1. 輸入句子給 `encoder` 獲得 `hidden state`，準備給 `decoder` 計算 `attention`。\n","2. 定義初始輸入 `<BOS>`。\n","\n","**以下進入迴圈**\n","\n","3. 使用 `<BOS>` 輸入給 `decoder` 獲得一個預測詞。\n","4. 預測詞與真實詞計算損失。\n","5. 使用 `tearch forcing` 策略，將真實詞丟回給模型繼續預測，而不是把預測詞丟回給模型。\n","\n","**以上為迴圈**\n","\n","6. 拿出模型所有參數以及損失，使用 `optimizer` 進行梯度下降。\n"]},{"cell_type":"code","metadata":{"id":"lcVdCO_E6WwP"},"source":["def train_step(inp, tar, decoder):\n","\n","    loss = 0\n","    \"\"\"\n","    使用 with tf.GradientTape() 來告訴 tensorflow 以下的執行過程都會涉及到梯度的更新\n","    \"\"\"\n","    with tf.GradientTape() as tape:\n","        # 首先產生 encoder 所有的 hidden states\n","        states, last_state = encoder(inp)\n","        \n","        # 使用 encoder 的最後一步 last_state 作為 decoder 的 initial state\n","        hidden_state = [last_state, last_state]\n","        \n","        # 因為一開始還沒有正確答案，所以必須輸入給 decoder 一個起始的 token\n","        # tokenizer_sp.vocab_size: <BOS> token\n","        dec_input = tf.expand_dims([tokenizer_sp.vocab_size] * batch_size, axis=1)\n","        \n","        # auto-regressive 迴圈，每次預測出一個詞\n","        for t in range(1, tar.shape[1]):\n","            # decoder 進行預測 \n","            predictions, hidden_state = decoder(dec_input, hidden_state)\n","\n","            # 預測結果和標籤計算損失\n","            loss += loss_function(tar[:, t], predictions)\n","            \n","            # 使用 teacher forcing 策略，每次輸入給模型真實答案，而不是預測值，這樣做的原因是防止模型一直錯下去\n","            dec_input = tf.expand_dims(tar[:, t], axis=1)\n","\n","    # 計算一個 batch 中的\b\b\b平均損失\n","    batch_loss = (loss / int(tar.shape[1]))\n","\n","    # 拿出模型所有參數\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    # 計算模型參數的 gradient\n","    gradients = tape.gradient(loss, variables)\n","\n","    # Pre-normalized gradient\n","    (gradients, _) = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n","\n","    # 使用 gradient 進行梯度下降\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kmi6wO7AV2JS"},"source":["## Evaluation step\n","\n","\b定義評估方式，\u001d輸入為英文句子，輸出為預測西文。\n","\n","基本上與 `train_step` 雷同，差別在於兩個：\n","\n","1. 沒有進行梯度下降\n","2. 當模型預測出 `<EOS>` 時，即停止預測。"]},{"cell_type":"code","metadata":{"id":"WAyV_z8S6Wwa"},"source":["def evaluate(inp_sentence, decoder):\n","    \n","    # 輸入英文句子進行文字前處理\n","    inp_sentence = preprocess_sentence(inp_sentence)\n","\n","    # 英文句子轉換為 token\n","    inp_tokenized = tokenizer_en.encode(inp_sentence)\n","\n","    # 在英文句子前後加上 <BOS> 以及 <EOS>\n","    inp_id = [tokenizer_en.vocab_size] + inp_tokenized + [tokenizer_en.vocab_size+1]\n","\n","    # 新增一個 batch_size 維度，符合模型輸入\n","    inp_id = tf.expand_dims(inp_id, axis=0)\n","    \n","    # 定義 decoder 輸入 <BOS>\n","    dec_inp = tf.expand_dims([tokenizer_sp.vocab_size] * inp_id.shape[0], axis=0)\n","    \n","    # 產生 encoder 所有的 hidden states\n","    states, last_state = encoder(inp_id)\n","    hidden_state = [last_state, last_state]\n","    \n","    # 紀錄預測結果\n","    preds_ids = list()\n","            \n","    # 此迴圈開始進行預測，直到 max_length 或是預測出 <EOS> 就停止\n","    for t in range(max_length):\n","        # decoder 進行預測\n","        preds, hidden_state = decoder(dec_inp, hidden_state)\n","\n","        # 使用 argmax 挑出預測概率最高的 token\n","        preds_id = tf.argmax(preds[0], axis=0).numpy()\n","        \n","        # 新增一個 `batch_size` 維度，符合模型輸入\n","        dec_inp = tf.expand_dims([preds_id], axis=0)\n","        \n","        # 紀錄預測結果\n","        preds_ids.append(preds_id)\n","\n","        # 如果預測出 <BOS> 就跳出此迴圈\n","        if preds_id == tokenizer_sp.vocab_size + 1:\n","          break\n","\n","    # 去除非 <BOS> 和 <EOS> 的 token\n","    preds_ids = [ids for ids in preds_ids if ids < tokenizer_sp.vocab_size]\n","    \n","    # 將 token 還原成字串\n","    preds_sent = tokenizer_sp.decode(preds_ids)\n","\n","    return preds_sent, inp_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcsXoGFb6WwY"},"source":["## Training\n","\n","開始訓練過程"]},{"cell_type":"code","metadata":{"id":"j22Sp0bZ6WwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604943987222,"user_tz":-480,"elapsed":1181733,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"f78b5cd2-105a-407a-8034-946f98057393"},"source":["for epoch in tqdm.tqdm(tf.range(epochs)):\n","    start = time.time()\n","    \n","    total_loss = 0\n","    \n","    for (batch, (inp, tar)) in enumerate(train_dataset):\n","        # 如果輸入的句子數量不滿一個 `batch_size`，就複製最後一個句子直到達到 `batch_size`\n","        if inp.shape[0] != batch_size:\n","            repeats = batch_size - inp.shape[0]\n","            \n","            rep_inp = tf.convert_to_tensor(np.repeat(inp[-1:,:], repeats=repeats, axis=0))\n","            inp = tf.concat([inp, rep_inp], axis=0)\n","            \n","            rep_tar = tf.convert_to_tensor(np.repeat(tar[-1:,:], repeats=repeats, axis=0))\n","            tar = tf.concat([tar,rep_tar], axis=0)\n","            \n","        batch_loss = train_step(inp, tar, decoder)\n","        \n","        total_loss += batch_loss\n","        \n","        if batch % 50 == 0:\n","            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n","            \n","    if (epoch + 1) % 2 == 0:\n","        print('Save Model!')\n","        ckpt_manager.save()\n","\n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / (batch+1)))\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    result, sentence = evaluate('I am hungry.', decoder)\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: %s' % (result))\n","    print('-'*20)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 3.0197\n","Epoch 1 Batch 50 Loss 3.0482\n","Epoch 1 Batch 100 Loss 2.2492\n","Epoch 1 Batch 150 Loss 2.8754\n","Epoch 1 Batch 200 Loss 2.9355\n","Epoch 1 Batch 250 Loss 2.4273\n","Epoch 1 Batch 300 Loss 2.7904\n","Epoch 1 Batch 350 Loss 2.2738\n","Epoch 1 Batch 400 Loss 2.4360\n","Epoch 1 Batch 450 Loss 1.8827\n","Epoch 1 Batch 500 Loss 2.4871\n","Epoch 1 Batch 550 Loss 2.4378\n","Epoch 1 Batch 600 Loss 2.3619\n","Epoch 1 Batch 650 Loss 1.5520\n","Epoch 1 Batch 700 Loss 2.3545\n","Epoch 1 Batch 750 Loss 2.1930\n","Epoch 1 Batch 800 Loss 2.3394\n","Epoch 1 Batch 850 Loss 2.2697\n","Epoch 1 Batch 900 Loss 2.2092\n","Epoch 1 Batch 950 Loss 2.2289\n","Epoch 1 Batch 1000 Loss 1.9253\n","Epoch 1 Batch 1050 Loss 2.2208\n","Epoch 1 Batch 1100 Loss 1.8076\n","Epoch 1 Batch 1150 Loss 1.8301\n","Epoch 1 Batch 1200 Loss 2.3524\n","Epoch 1 Batch 1250 Loss 1.6508\n","Epoch 1 Batch 1300 Loss 2.7199\n","Epoch 1 Batch 1350 Loss 1.6651\n","Epoch 1 Batch 1400 Loss 2.2500\n","Epoch 1 Batch 1450 Loss 2.0988\n","Epoch 1 Batch 1500 Loss 2.0808\n","Epoch 1 Batch 1550 Loss 2.1355\n","Epoch 1 Batch 1600 Loss 1.8538\n","Epoch 1 Batch 1650 Loss 2.0625\n","Epoch 1 Loss 2.2218\n","Time taken for 1 epoch 833.0560624599457 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","  5%|▌         | 1/20 [13:53<4:23:52, 833.30s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es el es \n","--------------------\n","Epoch 2 Batch 0 Loss 2.1947\n","Epoch 2 Batch 50 Loss 1.8193\n","Epoch 2 Batch 100 Loss 1.4726\n","Epoch 2 Batch 150 Loss 1.8873\n","Epoch 2 Batch 200 Loss 2.1290\n","Epoch 2 Batch 250 Loss 1.6640\n","Epoch 2 Batch 300 Loss 1.5082\n","Epoch 2 Batch 350 Loss 1.9386\n","Epoch 2 Batch 400 Loss 1.6389\n","Epoch 2 Batch 450 Loss 1.7909\n","Epoch 2 Batch 500 Loss 1.8571\n","Epoch 2 Batch 550 Loss 1.9686\n","Epoch 2 Batch 600 Loss 1.3576\n","Epoch 2 Batch 650 Loss 1.7869\n","Epoch 2 Batch 700 Loss 1.9336\n","Epoch 2 Batch 750 Loss 1.4993\n","Epoch 2 Batch 800 Loss 1.4318\n","Epoch 2 Batch 850 Loss 1.4867\n","Epoch 2 Batch 900 Loss 1.4031\n","Epoch 2 Batch 950 Loss 2.0272\n","Epoch 2 Batch 1000 Loss 1.7339\n","Epoch 2 Batch 1050 Loss 1.5601\n","Epoch 2 Batch 1100 Loss 1.6037\n","Epoch 2 Batch 1150 Loss 1.8273\n","Epoch 2 Batch 1200 Loss 1.8662\n","Epoch 2 Batch 1250 Loss 1.2605\n","Epoch 2 Batch 1300 Loss 1.1046\n","Epoch 2 Batch 1350 Loss 1.7604\n","Epoch 2 Batch 1400 Loss 1.4390\n","Epoch 2 Batch 1450 Loss 1.8123\n","Epoch 2 Batch 1500 Loss 1.6572\n","Epoch 2 Batch 1550 Loss 1.8574\n","Epoch 2 Batch 1600 Loss 1.1063\n","Epoch 2 Batch 1650 Loss 1.0549\n","Save Model!\n","Epoch 2 Loss 1.7031\n","Time taken for 1 epoch 833.5954113006592 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 10%|█         | 2/20 [27:47<4:10:02, 833.47s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: me siento de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi de mi \n","--------------------\n","Epoch 3 Batch 0 Loss 1.3279\n","Epoch 3 Batch 50 Loss 1.0431\n","Epoch 3 Batch 100 Loss 1.2602\n","Epoch 3 Batch 150 Loss 1.2742\n","Epoch 3 Batch 200 Loss 1.3052\n","Epoch 3 Batch 250 Loss 1.8326\n","Epoch 3 Batch 300 Loss 1.5773\n","Epoch 3 Batch 350 Loss 1.3219\n","Epoch 3 Batch 400 Loss 1.3477\n","Epoch 3 Batch 450 Loss 1.3178\n","Epoch 3 Batch 500 Loss 1.5348\n","Epoch 3 Batch 550 Loss 1.6523\n","Epoch 3 Batch 600 Loss 1.2127\n","Epoch 3 Batch 650 Loss 1.2335\n","Epoch 3 Batch 700 Loss 0.8563\n","Epoch 3 Batch 750 Loss 1.4674\n","Epoch 3 Batch 800 Loss 1.6119\n","Epoch 3 Batch 850 Loss 1.5323\n","Epoch 3 Batch 900 Loss 1.1348\n","Epoch 3 Batch 950 Loss 0.9605\n","Epoch 3 Batch 1000 Loss 1.0583\n","Epoch 3 Batch 1050 Loss 1.3144\n","Epoch 3 Batch 1100 Loss 1.1279\n","Epoch 3 Batch 1150 Loss 1.4836\n","Epoch 3 Batch 1200 Loss 1.1074\n","Epoch 3 Batch 1250 Loss 1.4222\n","Epoch 3 Batch 1300 Loss 1.3086\n","Epoch 3 Batch 1350 Loss 1.1792\n","Epoch 3 Batch 1400 Loss 1.2495\n","Epoch 3 Batch 1450 Loss 1.1941\n","Epoch 3 Batch 1500 Loss 1.3507\n","Epoch 3 Batch 1550 Loss 1.2467\n","Epoch 3 Batch 1600 Loss 1.0486\n","Epoch 3 Batch 1650 Loss 0.8629\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 15%|█▌        | 3/20 [41:38<3:55:56, 832.71s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3 Loss 1.3599\n","Time taken for 1 epoch 830.9114964008331 sec\n","\n","Input: i am hungry.\n","Predicted translation: hoy estoy muy de hambre.\n","--------------------\n","Epoch 4 Batch 0 Loss 1.0405\n","Epoch 4 Batch 50 Loss 1.2972\n","Epoch 4 Batch 100 Loss 0.9874\n","Epoch 4 Batch 150 Loss 1.2532\n","Epoch 4 Batch 200 Loss 1.4359\n","Epoch 4 Batch 250 Loss 1.2003\n","Epoch 4 Batch 300 Loss 0.9684\n","Epoch 4 Batch 350 Loss 1.3970\n","Epoch 4 Batch 400 Loss 1.0255\n","Epoch 4 Batch 450 Loss 1.4267\n","Epoch 4 Batch 500 Loss 1.1360\n","Epoch 4 Batch 550 Loss 1.4456\n","Epoch 4 Batch 600 Loss 0.9346\n","Epoch 4 Batch 650 Loss 1.2365\n","Epoch 4 Batch 700 Loss 1.4381\n","Epoch 4 Batch 750 Loss 1.1315\n","Epoch 4 Batch 800 Loss 1.2751\n","Epoch 4 Batch 850 Loss 1.1129\n","Epoch 4 Batch 900 Loss 1.1902\n","Epoch 4 Batch 950 Loss 1.3255\n","Epoch 4 Batch 1000 Loss 1.0045\n","Epoch 4 Batch 1050 Loss 1.2285\n","Epoch 4 Batch 1100 Loss 0.8902\n","Epoch 4 Batch 1150 Loss 0.9575\n","Epoch 4 Batch 1200 Loss 1.0780\n","Epoch 4 Batch 1250 Loss 1.2316\n","Epoch 4 Batch 1300 Loss 0.8927\n","Epoch 4 Batch 1350 Loss 1.1212\n","Epoch 4 Batch 1400 Loss 0.8896\n","Epoch 4 Batch 1450 Loss 0.9941\n","Epoch 4 Batch 1500 Loss 1.2545\n","Epoch 4 Batch 1550 Loss 0.7288\n","Epoch 4 Batch 1600 Loss 1.0303\n","Epoch 4 Batch 1650 Loss 0.8866\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 20%|██        | 4/20 [55:33<3:42:14, 833.40s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 4 Loss 1.1167\n","Time taken for 1 epoch 834.9470210075378 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo un poco de hambre.\n","--------------------\n","Epoch 5 Batch 0 Loss 1.0960\n","Epoch 5 Batch 50 Loss 1.1548\n","Epoch 5 Batch 100 Loss 1.0415\n","Epoch 5 Batch 150 Loss 0.7405\n","Epoch 5 Batch 200 Loss 0.9987\n","Epoch 5 Batch 250 Loss 1.0153\n","Epoch 5 Batch 300 Loss 0.6127\n","Epoch 5 Batch 350 Loss 1.2661\n","Epoch 5 Batch 400 Loss 0.8691\n","Epoch 5 Batch 450 Loss 1.2279\n","Epoch 5 Batch 500 Loss 1.1368\n","Epoch 5 Batch 550 Loss 1.0363\n","Epoch 5 Batch 600 Loss 0.9660\n","Epoch 5 Batch 650 Loss 1.0456\n","Epoch 5 Batch 700 Loss 0.8545\n","Epoch 5 Batch 750 Loss 0.8876\n","Epoch 5 Batch 800 Loss 0.9563\n","Epoch 5 Batch 850 Loss 0.8867\n","Epoch 5 Batch 900 Loss 0.9110\n","Epoch 5 Batch 950 Loss 1.1923\n","Epoch 5 Batch 1000 Loss 0.7647\n","Epoch 5 Batch 1050 Loss 1.0653\n","Epoch 5 Batch 1100 Loss 0.8744\n","Epoch 5 Batch 1150 Loss 0.8691\n","Epoch 5 Batch 1200 Loss 0.6997\n","Epoch 5 Batch 1250 Loss 0.6503\n","Epoch 5 Batch 1300 Loss 0.8652\n","Epoch 5 Batch 1350 Loss 0.9643\n","Epoch 5 Batch 1400 Loss 0.4978\n","Epoch 5 Batch 1450 Loss 0.8952\n","Epoch 5 Batch 1500 Loss 0.8395\n","Epoch 5 Batch 1550 Loss 0.6685\n","Epoch 5 Batch 1600 Loss 0.8768\n","Epoch 5 Batch 1650 Loss 1.0955\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 25%|██▌       | 5/20 [1:09:22<3:28:02, 832.14s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 5 Loss 0.9404\n","Time taken for 1 epoch 829.1662862300873 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 6 Batch 0 Loss 0.7488\n","Epoch 6 Batch 50 Loss 0.6772\n","Epoch 6 Batch 100 Loss 0.8455\n","Epoch 6 Batch 150 Loss 0.7431\n","Epoch 6 Batch 200 Loss 1.0757\n","Epoch 6 Batch 250 Loss 0.7117\n","Epoch 6 Batch 300 Loss 0.7121\n","Epoch 6 Batch 350 Loss 0.6591\n","Epoch 6 Batch 400 Loss 0.6449\n","Epoch 6 Batch 450 Loss 0.8083\n","Epoch 6 Batch 500 Loss 0.9395\n","Epoch 6 Batch 550 Loss 0.7555\n","Epoch 6 Batch 600 Loss 0.8676\n","Epoch 6 Batch 650 Loss 0.7994\n","Epoch 6 Batch 700 Loss 0.8884\n","Epoch 6 Batch 750 Loss 0.5577\n","Epoch 6 Batch 800 Loss 0.8838\n","Epoch 6 Batch 850 Loss 0.9310\n","Epoch 6 Batch 900 Loss 0.6778\n","Epoch 6 Batch 950 Loss 0.8795\n","Epoch 6 Batch 1000 Loss 0.9135\n","Epoch 6 Batch 1050 Loss 0.5769\n","Epoch 6 Batch 1100 Loss 0.8629\n","Epoch 6 Batch 1150 Loss 0.5128\n","Epoch 6 Batch 1200 Loss 0.6634\n","Epoch 6 Batch 1250 Loss 0.5670\n","Epoch 6 Batch 1300 Loss 0.6092\n","Epoch 6 Batch 1350 Loss 0.8546\n","Epoch 6 Batch 1400 Loss 0.5952\n","Epoch 6 Batch 1450 Loss 0.8126\n","Epoch 6 Batch 1500 Loss 0.7928\n","Epoch 6 Batch 1550 Loss 0.6467\n","Epoch 6 Batch 1600 Loss 0.6726\n","Epoch 6 Batch 1650 Loss 0.9457\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 30%|███       | 6/20 [1:23:12<3:14:01, 831.53s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 6 Loss 0.8000\n","Time taken for 1 epoch 830.0832040309906 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 7 Batch 0 Loss 0.7184\n","Epoch 7 Batch 50 Loss 0.7816\n","Epoch 7 Batch 100 Loss 0.7736\n","Epoch 7 Batch 150 Loss 0.6671\n","Epoch 7 Batch 200 Loss 0.6230\n","Epoch 7 Batch 250 Loss 0.7245\n","Epoch 7 Batch 300 Loss 0.8476\n","Epoch 7 Batch 350 Loss 0.7354\n","Epoch 7 Batch 400 Loss 0.4558\n","Epoch 7 Batch 450 Loss 0.8819\n","Epoch 7 Batch 500 Loss 0.6589\n","Epoch 7 Batch 550 Loss 0.5001\n","Epoch 7 Batch 600 Loss 0.8309\n","Epoch 7 Batch 650 Loss 0.5647\n","Epoch 7 Batch 700 Loss 0.6934\n","Epoch 7 Batch 750 Loss 0.7174\n","Epoch 7 Batch 800 Loss 0.5848\n","Epoch 7 Batch 850 Loss 0.4216\n","Epoch 7 Batch 900 Loss 0.6696\n","Epoch 7 Batch 950 Loss 0.6939\n","Epoch 7 Batch 1000 Loss 0.6788\n","Epoch 7 Batch 1050 Loss 0.6325\n","Epoch 7 Batch 1100 Loss 0.4505\n","Epoch 7 Batch 1150 Loss 0.5914\n","Epoch 7 Batch 1200 Loss 0.7009\n","Epoch 7 Batch 1250 Loss 0.6959\n","Epoch 7 Batch 1300 Loss 0.8022\n","Epoch 7 Batch 1350 Loss 0.6857\n","Epoch 7 Batch 1400 Loss 0.6852\n","Epoch 7 Batch 1450 Loss 0.4476\n","Epoch 7 Batch 1500 Loss 0.6818\n","Epoch 7 Batch 1550 Loss 0.6467\n","Epoch 7 Batch 1600 Loss 0.7424\n","Epoch 7 Batch 1650 Loss 0.7009\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 35%|███▌      | 7/20 [1:37:05<3:00:15, 831.95s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 7 Loss 0.6883\n","Time taken for 1 epoch 832.9204576015472 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo sed.\n","--------------------\n","Epoch 8 Batch 0 Loss 0.5037\n","Epoch 8 Batch 50 Loss 0.5683\n","Epoch 8 Batch 100 Loss 0.6821\n","Epoch 8 Batch 150 Loss 0.3830\n","Epoch 8 Batch 200 Loss 0.6722\n","Epoch 8 Batch 250 Loss 0.5629\n","Epoch 8 Batch 300 Loss 0.6232\n","Epoch 8 Batch 350 Loss 0.7386\n","Epoch 8 Batch 400 Loss 0.4222\n","Epoch 8 Batch 450 Loss 0.6232\n","Epoch 8 Batch 500 Loss 0.7634\n","Epoch 8 Batch 550 Loss 0.6351\n","Epoch 8 Batch 600 Loss 0.6618\n","Epoch 8 Batch 650 Loss 0.5279\n","Epoch 8 Batch 700 Loss 0.5824\n","Epoch 8 Batch 750 Loss 0.5638\n","Epoch 8 Batch 800 Loss 0.6483\n","Epoch 8 Batch 850 Loss 0.7355\n","Epoch 8 Batch 900 Loss 0.7046\n","Epoch 8 Batch 950 Loss 0.5788\n","Epoch 8 Batch 1000 Loss 0.5152\n","Epoch 8 Batch 1050 Loss 0.5699\n","Epoch 8 Batch 1100 Loss 0.7179\n","Epoch 8 Batch 1150 Loss 0.6164\n","Epoch 8 Batch 1200 Loss 0.4782\n","Epoch 8 Batch 1250 Loss 0.5196\n","Epoch 8 Batch 1300 Loss 0.6344\n","Epoch 8 Batch 1350 Loss 0.7036\n","Epoch 8 Batch 1400 Loss 0.7616\n","Epoch 8 Batch 1450 Loss 0.6439\n","Epoch 8 Batch 1500 Loss 0.5530\n","Epoch 8 Batch 1550 Loss 0.5129\n","Epoch 8 Batch 1600 Loss 0.6356\n","Epoch 8 Batch 1650 Loss 0.6547\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 40%|████      | 8/20 [1:50:59<2:46:29, 832.47s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 8 Loss 0.5905\n","Time taken for 1 epoch 833.6507318019867 sec\n","\n","Input: i am hungry.\n","Predicted translation: . tengo mucho hambriento.\n","--------------------\n","Epoch 9 Batch 0 Loss 0.4199\n","Epoch 9 Batch 50 Loss 0.4991\n","Epoch 9 Batch 100 Loss 0.4199\n","Epoch 9 Batch 150 Loss 0.4834\n","Epoch 9 Batch 200 Loss 0.4941\n","Epoch 9 Batch 250 Loss 0.5025\n","Epoch 9 Batch 300 Loss 0.6776\n","Epoch 9 Batch 350 Loss 0.4432\n","Epoch 9 Batch 400 Loss 0.5432\n","Epoch 9 Batch 450 Loss 0.5459\n","Epoch 9 Batch 500 Loss 0.5496\n","Epoch 9 Batch 550 Loss 0.2950\n","Epoch 9 Batch 600 Loss 0.4084\n","Epoch 9 Batch 650 Loss 0.5291\n","Epoch 9 Batch 700 Loss 0.3878\n","Epoch 9 Batch 750 Loss 0.5790\n","Epoch 9 Batch 800 Loss 0.3846\n","Epoch 9 Batch 850 Loss 0.4079\n","Epoch 9 Batch 900 Loss 0.5414\n","Epoch 9 Batch 950 Loss 0.5831\n","Epoch 9 Batch 1000 Loss 0.6507\n","Epoch 9 Batch 1050 Loss 0.4886\n","Epoch 9 Batch 1100 Loss 0.6525\n","Epoch 9 Batch 1150 Loss 0.6392\n","Epoch 9 Batch 1200 Loss 0.5316\n","Epoch 9 Batch 1250 Loss 0.5712\n","Epoch 9 Batch 1300 Loss 0.4785\n","Epoch 9 Batch 1350 Loss 0.3386\n","Epoch 9 Batch 1400 Loss 0.3855\n","Epoch 9 Batch 1450 Loss 0.4879\n","Epoch 9 Batch 1500 Loss 0.4495\n","Epoch 9 Batch 1550 Loss 0.3983\n","Epoch 9 Batch 1600 Loss 0.5606\n","Epoch 9 Batch 1650 Loss 0.4468\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 45%|████▌     | 9/20 [2:04:52<2:32:39, 832.68s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 9 Loss 0.5083\n","Time taken for 1 epoch 833.1216471195221 sec\n","\n","Input: i am hungry.\n","Predicted translation: . tengo tres hambriento.\n","--------------------\n","Epoch 10 Batch 0 Loss 0.4062\n","Epoch 10 Batch 50 Loss 0.5377\n","Epoch 10 Batch 100 Loss 0.3661\n","Epoch 10 Batch 150 Loss 0.6067\n","Epoch 10 Batch 200 Loss 0.4001\n","Epoch 10 Batch 250 Loss 0.3902\n","Epoch 10 Batch 300 Loss 0.3806\n","Epoch 10 Batch 350 Loss 0.4064\n","Epoch 10 Batch 400 Loss 0.4916\n","Epoch 10 Batch 450 Loss 0.4030\n","Epoch 10 Batch 500 Loss 0.5440\n","Epoch 10 Batch 550 Loss 0.4939\n","Epoch 10 Batch 600 Loss 0.4461\n","Epoch 10 Batch 650 Loss 0.5344\n","Epoch 10 Batch 700 Loss 0.4663\n","Epoch 10 Batch 750 Loss 0.3803\n","Epoch 10 Batch 800 Loss 0.4210\n","Epoch 10 Batch 850 Loss 0.4480\n","Epoch 10 Batch 900 Loss 0.4590\n","Epoch 10 Batch 950 Loss 0.6042\n","Epoch 10 Batch 1000 Loss 0.5048\n","Epoch 10 Batch 1050 Loss 0.3846\n","Epoch 10 Batch 1100 Loss 0.4651\n","Epoch 10 Batch 1150 Loss 0.4087\n","Epoch 10 Batch 1200 Loss 0.4203\n","Epoch 10 Batch 1250 Loss 0.3156\n","Epoch 10 Batch 1300 Loss 0.3258\n","Epoch 10 Batch 1350 Loss 0.5302\n","Epoch 10 Batch 1400 Loss 0.3999\n","Epoch 10 Batch 1450 Loss 0.4672\n","Epoch 10 Batch 1500 Loss 0.5223\n","Epoch 10 Batch 1550 Loss 0.5385\n","Epoch 10 Batch 1600 Loss 0.3798\n","Epoch 10 Batch 1650 Loss 0.4362\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 50%|█████     | 10/20 [2:18:48<2:18:58, 833.88s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 10 Loss 0.4348\n","Time taken for 1 epoch 836.6505455970764 sec\n","\n","Input: i am hungry.\n","Predicted translation: . tengo hambriento.\n","--------------------\n","Epoch 11 Batch 0 Loss 0.4131\n","Epoch 11 Batch 50 Loss 0.4709\n","Epoch 11 Batch 100 Loss 0.4247\n","Epoch 11 Batch 150 Loss 0.4063\n","Epoch 11 Batch 200 Loss 0.3599\n","Epoch 11 Batch 250 Loss 0.3849\n","Epoch 11 Batch 300 Loss 0.4355\n","Epoch 11 Batch 350 Loss 0.3635\n","Epoch 11 Batch 400 Loss 0.3274\n","Epoch 11 Batch 450 Loss 0.4122\n","Epoch 11 Batch 500 Loss 0.3759\n","Epoch 11 Batch 550 Loss 0.3236\n","Epoch 11 Batch 600 Loss 0.3724\n","Epoch 11 Batch 650 Loss 0.3956\n","Epoch 11 Batch 700 Loss 0.4446\n","Epoch 11 Batch 750 Loss 0.4246\n","Epoch 11 Batch 800 Loss 0.3833\n","Epoch 11 Batch 850 Loss 0.3251\n","Epoch 11 Batch 900 Loss 0.3154\n","Epoch 11 Batch 950 Loss 0.4383\n","Epoch 11 Batch 1000 Loss 0.3120\n","Epoch 11 Batch 1050 Loss 0.3453\n","Epoch 11 Batch 1100 Loss 0.3273\n","Epoch 11 Batch 1150 Loss 0.3082\n","Epoch 11 Batch 1200 Loss 0.3809\n","Epoch 11 Batch 1250 Loss 0.3872\n","Epoch 11 Batch 1300 Loss 0.3723\n","Epoch 11 Batch 1350 Loss 0.3183\n","Epoch 11 Batch 1400 Loss 0.3972\n","Epoch 11 Batch 1450 Loss 0.3820\n","Epoch 11 Batch 1500 Loss 0.4265\n","Epoch 11 Batch 1550 Loss 0.3666\n","Epoch 11 Batch 1600 Loss 0.3061\n","Epoch 11 Batch 1650 Loss 0.4336\n","Epoch 11 Loss 0.3749\n","Time taken for 1 epoch 835.224417924881 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 55%|█████▌    | 11/20 [2:32:44<2:05:09, 834.36s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy \n","--------------------\n","Epoch 12 Batch 0 Loss 0.4046\n","Epoch 12 Batch 50 Loss 0.3855\n","Epoch 12 Batch 100 Loss 0.3476\n","Epoch 12 Batch 150 Loss 0.4100\n","Epoch 12 Batch 200 Loss 0.3332\n","Epoch 12 Batch 250 Loss 0.3618\n","Epoch 12 Batch 300 Loss 0.2088\n","Epoch 12 Batch 350 Loss 0.3542\n","Epoch 12 Batch 400 Loss 0.3369\n","Epoch 12 Batch 450 Loss 0.2572\n","Epoch 12 Batch 500 Loss 0.2812\n","Epoch 12 Batch 550 Loss 0.3408\n","Epoch 12 Batch 600 Loss 0.3948\n","Epoch 12 Batch 650 Loss 0.3091\n","Epoch 12 Batch 700 Loss 0.3883\n","Epoch 12 Batch 750 Loss 0.3704\n","Epoch 12 Batch 800 Loss 0.3006\n","Epoch 12 Batch 850 Loss 0.4087\n","Epoch 12 Batch 900 Loss 0.2722\n","Epoch 12 Batch 950 Loss 0.3572\n","Epoch 12 Batch 1000 Loss 0.3620\n","Epoch 12 Batch 1050 Loss 0.2898\n","Epoch 12 Batch 1100 Loss 0.4450\n","Epoch 12 Batch 1150 Loss 0.3475\n","Epoch 12 Batch 1200 Loss 0.3261\n","Epoch 12 Batch 1250 Loss 0.3020\n","Epoch 12 Batch 1300 Loss 0.3884\n","Epoch 12 Batch 1350 Loss 0.3242\n","Epoch 12 Batch 1400 Loss 0.3174\n","Epoch 12 Batch 1450 Loss 0.3718\n","Epoch 12 Batch 1500 Loss 0.2710\n","Epoch 12 Batch 1550 Loss 0.2430\n","Epoch 12 Batch 1600 Loss 0.4223\n","Epoch 12 Batch 1650 Loss 0.4250\n","Save Model!\n","Epoch 12 Loss 0.3228\n","Time taken for 1 epoch 833.2118263244629 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 60%|██████    | 12/20 [2:46:37<1:51:12, 834.08s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy \n","--------------------\n","Epoch 13 Batch 0 Loss 0.2851\n","Epoch 13 Batch 50 Loss 0.2624\n","Epoch 13 Batch 100 Loss 0.2336\n","Epoch 13 Batch 150 Loss 0.2717\n","Epoch 13 Batch 200 Loss 0.3255\n","Epoch 13 Batch 250 Loss 0.2429\n","Epoch 13 Batch 300 Loss 0.3076\n","Epoch 13 Batch 350 Loss 0.2719\n","Epoch 13 Batch 400 Loss 0.2542\n","Epoch 13 Batch 450 Loss 0.3580\n","Epoch 13 Batch 500 Loss 0.2686\n","Epoch 13 Batch 550 Loss 0.2205\n","Epoch 13 Batch 600 Loss 0.3744\n","Epoch 13 Batch 650 Loss 0.3558\n","Epoch 13 Batch 700 Loss 0.2648\n","Epoch 13 Batch 750 Loss 0.3005\n","Epoch 13 Batch 800 Loss 0.3259\n","Epoch 13 Batch 850 Loss 0.3181\n","Epoch 13 Batch 900 Loss 0.3261\n","Epoch 13 Batch 950 Loss 0.2877\n","Epoch 13 Batch 1000 Loss 0.2651\n","Epoch 13 Batch 1050 Loss 0.2712\n","Epoch 13 Batch 1100 Loss 0.2457\n","Epoch 13 Batch 1150 Loss 0.2133\n","Epoch 13 Batch 1200 Loss 0.1957\n","Epoch 13 Batch 1250 Loss 0.2955\n","Epoch 13 Batch 1300 Loss 0.2565\n","Epoch 13 Batch 1350 Loss 0.2231\n","Epoch 13 Batch 1400 Loss 0.2617\n","Epoch 13 Batch 1450 Loss 0.2986\n","Epoch 13 Batch 1500 Loss 0.3394\n","Epoch 13 Batch 1550 Loss 0.2285\n","Epoch 13 Batch 1600 Loss 0.3248\n","Epoch 13 Batch 1650 Loss 0.1923\n","Epoch 13 Loss 0.2792\n","Time taken for 1 epoch 832.8486766815186 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 65%|██████▌   | 13/20 [3:00:30<1:37:16, 833.79s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy hambrientosamente estoy \n","--------------------\n","Epoch 14 Batch 0 Loss 0.3060\n","Epoch 14 Batch 50 Loss 0.2795\n","Epoch 14 Batch 100 Loss 0.2501\n","Epoch 14 Batch 150 Loss 0.2601\n","Epoch 14 Batch 200 Loss 0.2618\n","Epoch 14 Batch 250 Loss 0.3178\n","Epoch 14 Batch 300 Loss 0.2533\n","Epoch 14 Batch 350 Loss 0.2216\n","Epoch 14 Batch 400 Loss 0.2507\n","Epoch 14 Batch 450 Loss 0.2699\n","Epoch 14 Batch 500 Loss 0.2099\n","Epoch 14 Batch 550 Loss 0.2618\n","Epoch 14 Batch 600 Loss 0.2470\n","Epoch 14 Batch 650 Loss 0.2659\n","Epoch 14 Batch 700 Loss 0.1993\n","Epoch 14 Batch 750 Loss 0.2626\n","Epoch 14 Batch 800 Loss 0.1721\n","Epoch 14 Batch 850 Loss 0.2398\n","Epoch 14 Batch 900 Loss 0.2378\n","Epoch 14 Batch 950 Loss 0.2191\n","Epoch 14 Batch 1000 Loss 0.2737\n","Epoch 14 Batch 1050 Loss 0.1911\n","Epoch 14 Batch 1100 Loss 0.2345\n","Epoch 14 Batch 1150 Loss 0.2482\n","Epoch 14 Batch 1200 Loss 0.2759\n","Epoch 14 Batch 1250 Loss 0.2428\n","Epoch 14 Batch 1300 Loss 0.2287\n","Epoch 14 Batch 1350 Loss 0.2132\n","Epoch 14 Batch 1400 Loss 0.2180\n","Epoch 14 Batch 1450 Loss 0.2317\n","Epoch 14 Batch 1500 Loss 0.2656\n","Epoch 14 Batch 1550 Loss 0.2855\n","Epoch 14 Batch 1600 Loss 0.2824\n","Epoch 14 Batch 1650 Loss 0.2613\n","Save Model!\n","Epoch 14 Loss 0.2422\n","Time taken for 1 epoch 830.9347360134125 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 70%|███████   | 14/20 [3:14:22<1:23:18, 833.01s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambrientoqueria estoy hambriento\n","--------------------\n","Epoch 15 Batch 0 Loss 0.2753\n","Epoch 15 Batch 50 Loss 0.2252\n","Epoch 15 Batch 100 Loss 0.1355\n","Epoch 15 Batch 150 Loss 0.2198\n","Epoch 15 Batch 200 Loss 0.2002\n","Epoch 15 Batch 250 Loss 0.2053\n","Epoch 15 Batch 300 Loss 0.2236\n","Epoch 15 Batch 350 Loss 0.2897\n","Epoch 15 Batch 400 Loss 0.2176\n","Epoch 15 Batch 450 Loss 0.1971\n","Epoch 15 Batch 500 Loss 0.1804\n","Epoch 15 Batch 550 Loss 0.2230\n","Epoch 15 Batch 600 Loss 0.2508\n","Epoch 15 Batch 650 Loss 0.2065\n","Epoch 15 Batch 700 Loss 0.1989\n","Epoch 15 Batch 750 Loss 0.1832\n","Epoch 15 Batch 800 Loss 0.2975\n","Epoch 15 Batch 850 Loss 0.2241\n","Epoch 15 Batch 900 Loss 0.2202\n","Epoch 15 Batch 950 Loss 0.2116\n","Epoch 15 Batch 1000 Loss 0.1402\n","Epoch 15 Batch 1050 Loss 0.2693\n","Epoch 15 Batch 1100 Loss 0.1166\n","Epoch 15 Batch 1150 Loss 0.1541\n","Epoch 15 Batch 1200 Loss 0.1523\n","Epoch 15 Batch 1250 Loss 0.1988\n","Epoch 15 Batch 1300 Loss 0.2468\n","Epoch 15 Batch 1350 Loss 0.2165\n","Epoch 15 Batch 1400 Loss 0.2686\n","Epoch 15 Batch 1450 Loss 0.2081\n","Epoch 15 Batch 1500 Loss 0.2473\n","Epoch 15 Batch 1550 Loss 0.1941\n","Epoch 15 Batch 1600 Loss 0.2475\n","Epoch 15 Batch 1650 Loss 0.2101\n","Epoch 15 Loss 0.2116\n","Time taken for 1 epoch 834.1208922863007 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 75%|███████▌  | 15/20 [3:28:16<1:09:27, 833.42s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambrientosamente bebio hambriento\n","--------------------\n","Epoch 16 Batch 0 Loss 0.1938\n","Epoch 16 Batch 50 Loss 0.2586\n","Epoch 16 Batch 100 Loss 0.1813\n","Epoch 16 Batch 150 Loss 0.2136\n","Epoch 16 Batch 200 Loss 0.1829\n","Epoch 16 Batch 250 Loss 0.1954\n","Epoch 16 Batch 300 Loss 0.2261\n","Epoch 16 Batch 350 Loss 0.1576\n","Epoch 16 Batch 400 Loss 0.1551\n","Epoch 16 Batch 450 Loss 0.1785\n","Epoch 16 Batch 500 Loss 0.2029\n","Epoch 16 Batch 550 Loss 0.1882\n","Epoch 16 Batch 600 Loss 0.2319\n","Epoch 16 Batch 650 Loss 0.1693\n","Epoch 16 Batch 700 Loss 0.1975\n","Epoch 16 Batch 750 Loss 0.1602\n","Epoch 16 Batch 800 Loss 0.1751\n","Epoch 16 Batch 850 Loss 0.1944\n","Epoch 16 Batch 900 Loss 0.1632\n","Epoch 16 Batch 950 Loss 0.1434\n","Epoch 16 Batch 1000 Loss 0.1960\n","Epoch 16 Batch 1050 Loss 0.1981\n","Epoch 16 Batch 1100 Loss 0.1685\n","Epoch 16 Batch 1150 Loss 0.1809\n","Epoch 16 Batch 1200 Loss 0.1439\n","Epoch 16 Batch 1250 Loss 0.2326\n","Epoch 16 Batch 1300 Loss 0.1753\n","Epoch 16 Batch 1350 Loss 0.1510\n","Epoch 16 Batch 1400 Loss 0.1876\n","Epoch 16 Batch 1450 Loss 0.1733\n","Epoch 16 Batch 1500 Loss 0.1616\n","Epoch 16 Batch 1550 Loss 0.1535\n","Epoch 16 Batch 1600 Loss 0.1270\n","Epoch 16 Batch 1650 Loss 0.2088\n","Save Model!\n","Epoch 16 Loss 0.1872\n","Time taken for 1 epoch 831.9847724437714 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 80%|████████  | 16/20 [3:42:08<55:32, 833.06s/it]  \u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy \n","--------------------\n","Epoch 17 Batch 0 Loss 0.1985\n","Epoch 17 Batch 50 Loss 0.1672\n","Epoch 17 Batch 100 Loss 0.0956\n","Epoch 17 Batch 150 Loss 0.1301\n","Epoch 17 Batch 200 Loss 0.1927\n","Epoch 17 Batch 250 Loss 0.1435\n","Epoch 17 Batch 300 Loss 0.1469\n","Epoch 17 Batch 350 Loss 0.1493\n","Epoch 17 Batch 400 Loss 0.1473\n","Epoch 17 Batch 450 Loss 0.1683\n","Epoch 17 Batch 500 Loss 0.1708\n","Epoch 17 Batch 550 Loss 0.1658\n","Epoch 17 Batch 600 Loss 0.1467\n","Epoch 17 Batch 650 Loss 0.1838\n","Epoch 17 Batch 700 Loss 0.1703\n","Epoch 17 Batch 750 Loss 0.1594\n","Epoch 17 Batch 800 Loss 0.1290\n","Epoch 17 Batch 850 Loss 0.1469\n","Epoch 17 Batch 900 Loss 0.1062\n","Epoch 17 Batch 950 Loss 0.1234\n","Epoch 17 Batch 1000 Loss 0.2384\n","Epoch 17 Batch 1050 Loss 0.1603\n","Epoch 17 Batch 1100 Loss 0.1995\n","Epoch 17 Batch 1150 Loss 0.2286\n","Epoch 17 Batch 1200 Loss 0.1653\n","Epoch 17 Batch 1250 Loss 0.1475\n","Epoch 17 Batch 1300 Loss 0.1791\n","Epoch 17 Batch 1350 Loss 0.1363\n","Epoch 17 Batch 1400 Loss 0.2312\n","Epoch 17 Batch 1450 Loss 0.1637\n","Epoch 17 Batch 1500 Loss 0.1659\n","Epoch 17 Batch 1550 Loss 0.1608\n","Epoch 17 Batch 1600 Loss 0.1544\n","Epoch 17 Batch 1650 Loss 0.2031\n","Epoch 17 Loss 0.1669\n","Time taken for 1 epoch 830.6061215400696 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 85%|████████▌ | 17/20 [3:55:59<41:37, 832.40s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . bebio hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy hambrientomente estoy \n","--------------------\n","Epoch 18 Batch 0 Loss 0.2008\n","Epoch 18 Batch 50 Loss 0.1390\n","Epoch 18 Batch 100 Loss 0.1647\n","Epoch 18 Batch 150 Loss 0.1180\n","Epoch 18 Batch 200 Loss 0.1653\n","Epoch 18 Batch 250 Loss 0.1471\n","Epoch 18 Batch 300 Loss 0.1050\n","Epoch 18 Batch 350 Loss 0.1471\n","Epoch 18 Batch 400 Loss 0.1341\n","Epoch 18 Batch 450 Loss 0.2030\n","Epoch 18 Batch 500 Loss 0.2001\n","Epoch 18 Batch 550 Loss 0.2050\n","Epoch 18 Batch 600 Loss 0.1548\n","Epoch 18 Batch 650 Loss 0.1616\n","Epoch 18 Batch 700 Loss 0.1304\n","Epoch 18 Batch 750 Loss 0.1390\n","Epoch 18 Batch 800 Loss 0.1637\n","Epoch 18 Batch 850 Loss 0.1945\n","Epoch 18 Batch 900 Loss 0.1382\n","Epoch 18 Batch 950 Loss 0.1106\n","Epoch 18 Batch 1000 Loss 0.1418\n","Epoch 18 Batch 1050 Loss 0.1677\n","Epoch 18 Batch 1100 Loss 0.1614\n","Epoch 18 Batch 1150 Loss 0.1615\n","Epoch 18 Batch 1200 Loss 0.1361\n","Epoch 18 Batch 1250 Loss 0.1814\n","Epoch 18 Batch 1300 Loss 0.1637\n","Epoch 18 Batch 1350 Loss 0.1752\n","Epoch 18 Batch 1400 Loss 0.1649\n","Epoch 18 Batch 1450 Loss 0.1034\n","Epoch 18 Batch 1500 Loss 0.1553\n","Epoch 18 Batch 1550 Loss 0.1193\n","Epoch 18 Batch 1600 Loss 0.1840\n","Epoch 18 Batch 1650 Loss 0.1570\n","Save Model!\n","Epoch 18 Loss 0.1514\n","Time taken for 1 epoch 831.9368305206299 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 90%|█████████ | 18/20 [4:09:51<27:44, 832.33s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . castillo estoy vitaminataletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletaletale\n","--------------------\n","Epoch 19 Batch 0 Loss 0.1763\n","Epoch 19 Batch 50 Loss 0.2050\n","Epoch 19 Batch 100 Loss 0.1382\n","Epoch 19 Batch 150 Loss 0.1366\n","Epoch 19 Batch 200 Loss 0.1322\n","Epoch 19 Batch 250 Loss 0.1437\n","Epoch 19 Batch 300 Loss 0.1094\n","Epoch 19 Batch 350 Loss 0.1782\n","Epoch 19 Batch 400 Loss 0.1652\n","Epoch 19 Batch 450 Loss 0.1320\n","Epoch 19 Batch 500 Loss 0.1843\n","Epoch 19 Batch 550 Loss 0.1317\n","Epoch 19 Batch 600 Loss 0.1137\n","Epoch 19 Batch 650 Loss 0.0884\n","Epoch 19 Batch 700 Loss 0.1321\n","Epoch 19 Batch 750 Loss 0.1062\n","Epoch 19 Batch 800 Loss 0.1686\n","Epoch 19 Batch 850 Loss 0.1394\n","Epoch 19 Batch 900 Loss 0.1251\n","Epoch 19 Batch 950 Loss 0.1044\n","Epoch 19 Batch 1000 Loss 0.1064\n","Epoch 19 Batch 1050 Loss 0.1290\n","Epoch 19 Batch 1100 Loss 0.1385\n","Epoch 19 Batch 1150 Loss 0.1706\n","Epoch 19 Batch 1200 Loss 0.1547\n","Epoch 19 Batch 1250 Loss 0.0892\n","Epoch 19 Batch 1300 Loss 0.1552\n","Epoch 19 Batch 1350 Loss 0.1130\n","Epoch 19 Batch 1400 Loss 0.1437\n","Epoch 19 Batch 1450 Loss 0.1480\n","Epoch 19 Batch 1500 Loss 0.1429\n","Epoch 19 Batch 1550 Loss 0.1349\n","Epoch 19 Batch 1600 Loss 0.0927\n","Epoch 19 Batch 1650 Loss 0.1128\n","Epoch 19 Loss 0.1388\n","Time taken for 1 epoch 830.904093503952 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n"," 95%|█████████▌| 19/20 [4:23:42<13:51, 831.98s/it]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . montontalequeria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy vitaminaqueria estoy \n","--------------------\n","Epoch 20 Batch 0 Loss 0.1351\n","Epoch 20 Batch 50 Loss 0.1250\n","Epoch 20 Batch 100 Loss 0.1061\n","Epoch 20 Batch 150 Loss 0.1506\n","Epoch 20 Batch 200 Loss 0.1133\n","Epoch 20 Batch 250 Loss 0.1530\n","Epoch 20 Batch 300 Loss 0.1119\n","Epoch 20 Batch 350 Loss 0.1573\n","Epoch 20 Batch 400 Loss 0.1265\n","Epoch 20 Batch 450 Loss 0.1701\n","Epoch 20 Batch 500 Loss 0.1537\n","Epoch 20 Batch 550 Loss 0.1152\n","Epoch 20 Batch 600 Loss 0.0949\n","Epoch 20 Batch 650 Loss 0.1418\n","Epoch 20 Batch 700 Loss 0.1120\n","Epoch 20 Batch 750 Loss 0.1351\n","Epoch 20 Batch 800 Loss 0.1055\n","Epoch 20 Batch 850 Loss 0.0866\n","Epoch 20 Batch 900 Loss 0.1398\n","Epoch 20 Batch 950 Loss 0.0951\n","Epoch 20 Batch 1000 Loss 0.1398\n","Epoch 20 Batch 1050 Loss 0.1518\n","Epoch 20 Batch 1100 Loss 0.1288\n","Epoch 20 Batch 1150 Loss 0.1328\n","Epoch 20 Batch 1200 Loss 0.1140\n","Epoch 20 Batch 1250 Loss 0.1007\n","Epoch 20 Batch 1300 Loss 0.1289\n","Epoch 20 Batch 1350 Loss 0.1235\n","Epoch 20 Batch 1400 Loss 0.1465\n","Epoch 20 Batch 1450 Loss 0.1552\n","Epoch 20 Batch 1500 Loss 0.1089\n","Epoch 20 Batch 1550 Loss 0.1040\n","Epoch 20 Batch 1600 Loss 0.1170\n","Epoch 20 Batch 1650 Loss 0.1448\n","Save Model!\n","Epoch 20 Loss 0.1276\n","Time taken for 1 epoch 837.2874209880829 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","100%|██████████| 20/20 [4:37:40<00:00, 833.02s/it]"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: . castillo estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente estoy vitaminasamente \n","--------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"pPJWs8nsV_ac"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"y8yUoIGjQEtx"},"source":["### 載入模型\n","\n","這邊載入模型的方式是先建立模型，再把參數載入模型"]},{"cell_type":"code","metadata":{"id":"VqYSYhgLP--z"},"source":["model_name = 'checkpoints_seq2seq'\n","\n","# 建立模型\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units)\n","\n","# 模型資訊\n","checkpoint_path = os.path.join(output_dir, model_name)\n","ckpt = tf.train.Checkpoint(encoder = encoder, decoder = decoder, optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nN0Ql9zNlSJh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604944748673,"user_tz":-480,"elapsed":30266,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"6545d630-2341-4164-ddc5-390c59c89c18"},"source":["# 查看在資料夾底下有幾個模型\n","ckpt_manager.checkpoints"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['nmt_seq2seq/checkpoints_seq2seq/ckpt-8',\n"," 'nmt_seq2seq/checkpoints_seq2seq/ckpt-9',\n"," 'nmt_seq2seq/checkpoints_seq2seq/ckpt-10']"]},"metadata":{"tags":[]},"execution_count":141}]},{"cell_type":"code","metadata":{"id":"rOZi1gqHlT_q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604944779589,"user_tz":-480,"elapsed":4149,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"ed15c670-d33c-4aca-9003-ec57fb6acee3"},"source":["# 選擇載入哪個模型\n","ckpt.restore(ckpt_manager.checkpoints[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff64d780630>"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"markdown","metadata":{"id":"uqIjtqQgksZO"},"source":["### 預測效果\n","\n","西文翻譯網址: https://context.reverso.net/translation/spanish-english/estados+unidos+es+un+pais+mas+bonito"]},{"cell_type":"code","metadata":{"id":"2nR3A8vLJmps"},"source":["def translate(sentence, decoder):\n","  pred, sentence = evaluate(sentence, decoder)\n","\n","  print('Input:\\n %s' % (sentence))\n","  print('Predicted translation:\\n %s' % (pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLwdytxKJ-Iy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604945119937,"user_tz":-480,"elapsed":702,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"7c665ca8-a02b-457d-cf60-32d3634b3cf5"},"source":["inp_sentence = 'We must go.'\n","translate(inp_sentence, decoder)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\n"," we must go.\n","Predicted translation:\n"," podemos ir.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ArpQ-cRKLinR"},"source":[""],"execution_count":null,"outputs":[]}]}