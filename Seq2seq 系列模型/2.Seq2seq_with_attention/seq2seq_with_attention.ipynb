{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"seq2seq_with_attention.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"id":"eisd_CL4-maw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928212903,"user_tz":-480,"elapsed":759,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"35d18788-fb69-4f95-f5d3-51b2fbaec16c"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon Nov  9 13:23:32 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P0    37W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FLc5D2cU-pp7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928213503,"user_tz":-480,"elapsed":1344,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"d05a7c0a-e0bd-43ce-8d4f-4a6c36281a05"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VGrb5CvrGLDq"},"source":["* @file NLP進階 / Seq2seq_with_attention\n","  * @brief Seq2seq_with_attention 模型實作 \n","\n","  * 此份程式碼是以教學為目的，附有完整的架構解說。\n","\n","  * @author 人工智慧科技基金會 AI 工程師 - 康文瑋\n","  * Email: run963741@aif.tw\n","  * Resume: https://www.cakeresume.com/run963741\n","\n","  * 最後更新日期: 2020/11/13"]},{"cell_type":"markdown","metadata":{"id":"wVMXTZBCvk7m"},"source":["# Seq2seq with attention mechanism\n","\n","在 2014 年末，Bahdanau 等人提出了[注意力機制 (Attention mechanism)](https://arxiv.org/abs/1409.0473)，\u001d大幅度提升 `Seq2seq` 在機器翻譯上的表現，亦使得深度學習領域向前邁一大步，現在許多知名的模型 (CNN 方面、RNN 方面) 都是以此為基礎進行設計，甚至是目前自然語言處理最強模型 `Transformer` 也是以此為思想進行設計的。"]},{"cell_type":"markdown","metadata":{"id":"g87e-7LGvjXW"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=19OpGD0XFy1W-IOmBaPOMfv195vU-rr4O' width=\"800\"/>\n","<figcaption></figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"rkdslBbP6Wuh"},"source":["# Environment\n","\n","#### - Tensorflow 2.3.0\n","#### - python3.7"]},{"cell_type":"markdown","metadata":{"id":"T0RAfBBMy1tT"},"source":["# 載入套件"]},{"cell_type":"code","metadata":{"id":"JjJJyJTZYebt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928215807,"user_tz":-480,"elapsed":3632,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"0fa56c9a-9409-4142-b0b9-051b94ed0aba"},"source":["import tensorflow_datasets as tfds\n","import tensorflow_addons as tfa\n","import tensorflow as tf\n","import os\n","import tqdm\n","import unicodedata\n","import re\n","import io\n","from pprint import pprint\n","from sklearn.model_selection import train_test_split\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","os.chdir('/content/drive/Shared drives/類技術班教材/標準版/NLP進階/Seq2seq 系列模型/Seq2seq_with_attention')\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fd1NWMxjfsDd"},"source":["# 建立資料夾路徑\n","\n","`en_vocab_file`: 儲存英文字典 (vocabulary) 路徑\n","\n","`sp_vocab_file`: 儲存西文字典 (vocabulary) 路徑\n","\n","`checkpoint_path`: 儲存模型路徑\n","\n","`download_dir`: 資料儲存路徑"]},{"cell_type":"code","metadata":{"id":"4wKndea96Wum"},"source":["output_dir = \"nmt_seq2seq_with_attention\"\n","en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n","sp_vocab_file = os.path.join(output_dir, \"sp_vocab\")\n","download_dir = \"tensorflow-datasets/downloads\"\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","if not os.path.exists(download_dir):\n","    os.makedirs(download_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntjxFlUo6Wuv"},"source":["# 下載 `Tensorflow` 範例資料集\n","\n","下載英文和西班牙文的範例資料集。"]},{"cell_type":"code","metadata":{"id":"4IP6dH3d79pP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928215810,"user_tz":-480,"elapsed":3616,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"4904bb15-9757-4392-bb06-4b8d13fb8c18"},"source":["# Download the file\n","path_to_zip = tf.keras.utils.get_file(\n","    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True, cache_dir = download_dir)\n","\n","path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n","\n","print('--'*20)\n","print('資料路徑: ', path_to_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","資料路徑:  tensorflow-datasets/downloads/datasets/spa-eng/spa.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zbs5IG7kaiTM"},"source":["# 資料前處理"]},{"cell_type":"markdown","metadata":{"id":"vbo607Xdazgp"},"source":["## 字串處理\n","\n","在 Unicode 中，某些字符能夠用多種合法的底層編碼，例如在以下範例中，西班牙文字 $\\tilde{n}$ 可以由兩種編碼來表示，這種情況會導致後續建立語言模型時產生問題，所以我們要使用 `unicodedata` 將文字做標準化。"]},{"cell_type":"code","metadata":{"id":"1-PQrj8xbKVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928215812,"user_tz":-480,"elapsed":3605,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"1df01a10-45cc-4a6a-bf73-1c5f3706ae3c"},"source":["s1 = 'Spicy Jalape\\u00f1o'\n","s2 = 'Spicy Jalapen\\u0303o'\n","\n","print('s1: ', s1)\n","print('s2: ', s2)\n","print('s1 和 s2 是否一樣: ', s1 == s2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["s1:  Spicy Jalapeño\n","s2:  Spicy Jalapeño\n","s1 和 s2 是否一樣:  False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3vkiu10Exl1o"},"source":["### `unicodedata` 標準化範例"]},{"cell_type":"code","metadata":{"id":"dT605N4sxp90","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928215814,"user_tz":-480,"elapsed":3593,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"835cb3b2-f28d-472f-be1f-94e4d0490ad7"},"source":["# https://python3-cookbook.readthedocs.io/zh_CN/latest/c02/p09_normalize_unicode_text_to_regexp.html\n","s1_normalized = unicodedata.normalize('NFD', s1)\n","s2_normalized = unicodedata.normalize('NFD', s2)\n","\n","print('s1: ', s1_normalized)\n","print('s2: ', s2_normalized)\n","print('s1 和 s2 是否一樣: ', s1_normalized == s2_normalized)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["s1:  Spicy Jalapeño\n","s2:  Spicy Jalapeño\n","s1 和 s2 是否一樣:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ai75FUKW1Mq2"},"source":["## 字串處理函數"]},{"cell_type":"code","metadata":{"id":"PedIwQ2r8ZIl"},"source":["def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn') # Mn 判斷是否為 Nonspacing\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","\n","  # 正則表達式: https://www.runoob.com/regexp/regexp-syntax\n","  # http://ccckmit.wikidot.com/regularexpression\n","  w = re.sub(r\"([?.!,¿])\", r\"\\1\", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","  w = w.strip()\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0p8-lyHI8P20"},"source":["def create_dataset(path, num_examples):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","  \n","  # 使用 /t 把英文和西文分開\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","\n","  return zip(*word_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpGYs76O1ix4"},"source":["## 讀取資料集"]},{"cell_type":"code","metadata":{"id":"PU8kyLrm8mp6"},"source":["en, sp = create_dataset(path_to_file, None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnOaS2SrC7MR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928219798,"user_tz":-480,"elapsed":7540,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"2599c511-f3fb-4791-ce40-6d2439422225"},"source":["print(en[20])\n","print(sp[20])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["wait.\n","esperen.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k8IJqAQqKUgG"},"source":["## 切割訓練集 (Training) 和測試集 (Testing)"]},{"cell_type":"code","metadata":{"id":"gk2OVBfrKag4"},"source":["en_train, en_test, sp_train, sp_test = train_test_split(en, sp, test_size = 0.1, shuffle = True)\n","\n","train_examples = tf.data.Dataset.from_tensor_slices((en_train, sp_train))\n","test_examples = tf.data.Dataset.from_tensor_slices((en_test, sp_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e_xB4ekQOZZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221413,"user_tz":-480,"elapsed":9134,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"89fabc55-d58f-4c7d-d753-c3fc1a65cf90"},"source":["print('Train size: ', len(en_train))\n","print('Test size: ', len(en_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train size:  107067\n","Test size:  11897\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"03Kr0yj1LBST","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221414,"user_tz":-480,"elapsed":9123,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"7e95a1ab-4be2-4b02-938a-91d2e84c6c0a"},"source":["tmp_en, tmp_sp = next(iter(train_examples))\n","\n","print('Input english: ', tmp_en)\n","print('Output spanish: ', tmp_sp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input english:  tf.Tensor(b'it was about money.', shape=(), dtype=string)\n","Output spanish:  tf.Tensor(b'era cuestion de dinero.', shape=(), dtype=string)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y9e7Njms6Wu-"},"source":["## 使用`tfds.deprecated.text.SubwordTextEncoder`載入與建立字典\n","\n","* `.load_from_file`: 從路徑載入字典\n","* `.build_from_corpus`: 建立字典\n","* `.save_to_file`: 儲存字典"]},{"cell_type":"code","metadata":{"id":"8qYu_H9N6WvD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221416,"user_tz":-480,"elapsed":9114,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"c3769dad-42d5-48a4-e517-e910ca2657e6"},"source":["%%time\n","try: \n","    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file) \n","    print('Load English vocabulary: %s' % en_vocab_file)\n","except: \n","    print('Build English vocabulary: %s' % en_vocab_file)\n","    tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() for en, sp in train_examples),target_vocab_size = 2**13)\n","    tokenizer_en.save_to_file(en_vocab_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load English vocabulary: nmt_seq2seq_with_attention/en_vocab\n","CPU times: user 38.4 ms, sys: 71 µs, total: 38.4 ms\n","Wall time: 42.4 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KVBg5Q8tBk5z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221416,"user_tz":-480,"elapsed":9103,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"3372316c-6429-4d5f-80eb-87851514986b"},"source":["%%time\n","try: \n","    tokenizer_sp = tfds.deprecated.text.SubwordTextEncoder.load_from_file(sp_vocab_file) \n","    print('Load Spanish vocfabulary: %s' % sp_vocab_file)\n","except: \n","    print('Build Spanish vocabulary: %s' % sp_vocab_file)\n","    tokenizer_sp = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((sp.numpy() for en, sp in train_examples), target_vocab_size = 2**13)\n","    tokenizer_sp.save_to_file(sp_vocab_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load Spanish vocfabulary: nmt_seq2seq_with_attention/sp_vocab\n","CPU times: user 31.3 ms, sys: 5.04 ms, total: 36.3 ms\n","Wall time: 36.7 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rHDcuv8K6M79"},"source":["### 字典大小以及 `subword`"]},{"cell_type":"code","metadata":{"id":"1-xZvaYj6WvF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221417,"user_tz":-480,"elapsed":9093,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"87af03a4-174c-4d8a-d275-ccbba347aab2"},"source":["print('English vocabulary size: ', tokenizer_en.vocab_size)\n","print('Spanish vocabulary size: ', tokenizer_sp.vocab_size)\n","print('-' * 30)\n","print('English subwords: ', tokenizer_en.subwords[:10])\n","print('Spanish subwords: ', tokenizer_sp.subwords[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["English vocabulary size:  8285\n","Spanish vocabulary size:  8040\n","------------------------------\n","English subwords:  ['i_', 'the_', 'to_', 'you_', 'tom_', 'a_', 't_', 'is_', 'he_', 's_']\n","Spanish subwords:  ['que_', 'de_', 'a_', 'el_', 'no_', 'la_', 'tom_', '¿', 'es_', 'en_']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sEqjnOoz6WvH"},"source":["### Example\n","\n","英文的斷詞方式是以 `wordpiece` 進行斷詞。"]},{"cell_type":"code","metadata":{"id":"4DYWukNFkGQN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221417,"user_tz":-480,"elapsed":9082,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"8f1b88f4-91b1-4805-df5a-1a53461a3ce2"},"source":["sample_string = 'Transformer is awesome.'\n","\n","tokenized_string_token = tokenizer_en.encode(sample_string)\n","print ('Tokenized string token is \\n{}'.format(tokenized_string_token))\n","\n","print('-'*20)\n","tokenized_string = [tokenizer_en.decode([ts]) for ts in tokenized_string_token]\n","print('Tokenized srting is \\n{}'.format(tokenized_string))\n","\n","print('-'*20)\n","original_string = tokenizer_en.decode(tokenized_string_token)\n","print ('The original string: \\n{}'.format(original_string))\n","\n","assert original_string == sample_string"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tokenized string token is \n","[8113, 2193, 8144, 6770, 8, 4405, 8075]\n","--------------------\n","Tokenized srting is \n","['T', 'ran', 's', 'former ', 'is ', 'awesome', '.']\n","--------------------\n","The original string: \n","Transformer is awesome.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7cvjtgiO6WvL"},"source":["## 添加`<BOS>`,`<EOS>`在句子頭尾\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=11_d2u6W8t3qi_T8x22jKQ-MYnBNM6-JW' width=\"500\"/>\n","<figcaption>Auto Regressive</figcaption></center>\n","</figure>\n","\n","Seq2seq 模型的訓練方式以及預測方式都是使用 Auto Regressive 的模式來進行，將當前時間點 $T_1$ 的預測值接在下一個時間點 $T_2$ 的後面，再輸入給模型，直到模型預測出 $<EOS>$ 為止。\n","\n","* $<BOS>$: 全名為 Begin of sentence，因為第一個時間點 $T_1$ 翻譯時不可能馬上有正確答案，所以會統一輸入 $<BOS>$ ，例如上圖的要完整預測出 $文瑋助教真帥$，第一個時間點 $T_1$ 還沒有 $文$ 這個字，所以使用 $<BOS>$ 來作為輸入。\n","\n","* $<EOS>$: 全名為 End of sentence，當模型預測出這個 token 時，就代表整個序列預測完畢，如果前處理沒有加上 $<EOS>$，模型就會永無止盡的預測下去。"]},{"cell_type":"code","metadata":{"id":"UZwnPr4R055s"},"source":["def encode(en_t, sp_t):\n","    \"\"\"\n","    這邊將 `.vocab_size`視為`<BOS>`, `.vocab_size+1`視為`<EOS>`\n","    訓練集所有句子都需要進行這一步前處理\n","    \"\"\"\n","    en_indics = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy()) + [tokenizer_en.vocab_size + 1]\n","    sp_indics = [tokenizer_sp.vocab_size] + tokenizer_sp.encode(sp_t.numpy()) + [tokenizer_sp.vocab_size + 1]\n","\n","    return en_indics, sp_indics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ug-KgwUH6WvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221420,"user_tz":-480,"elapsed":9067,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"bd311139-29e6-4437-eb3e-3c34ae523921"},"source":["en_t, sp_t = next(iter(train_examples))\n","en_indics, sp_indics = encode(en_t, sp_t)\n","\n","print('英文<BOS>: %d' % tokenizer_en.vocab_size)\n","print('英文<EOS>: %d' % (tokenizer_en.vocab_size + 1))\n","print('西文<BOS>: %d' % tokenizer_sp.vocab_size)\n","print('西文<EOS>: %d' % (tokenizer_sp.vocab_size + 1))\n","\n","print('-' * 20)\n","print('Before encode: (two tensor):')\n","print(en_t)\n","print(sp_t)\n","print()\n","print('After encode: (two array): ')\n","print(en_indics)\n","print(sp_indics)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["英文<BOS>: 8285\n","英文<EOS>: 8286\n","西文<BOS>: 8040\n","西文<EOS>: 8041\n","--------------------\n","Before encode: (two tensor):\n","tf.Tensor(b'it was about money.', shape=(), dtype=string)\n","tf.Tensor(b'era cuestion de dinero.', shape=(), dtype=string)\n","\n","After encode: (two array): \n","[8285, 13, 14, 59, 172, 8075, 8286]\n","[8040, 55, 5647, 2, 153, 7830, 8041]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tx1sFbR-9fRs"},"source":["### `tf.py_function`\n","\n","在 Tensorflow 的訓練過程中，所有的計算過程都必須使用 `tf.` 來達成，當有某一些函數操作不涉及到 `tf.` 時，就必須使用 `tf.py_function` 來將函數納入 tensorflow 的計算流程裡面。"]},{"cell_type":"code","metadata":{"id":"By6N4r_16WvR"},"source":["# import traceback\n","\n","# try:\n","#     train_examples.map(encode)\n","# except AttributeError:\n","#     traceback.print_exc()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mah1cS-P70Iz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928221421,"user_tz":-480,"elapsed":9047,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"de05abb6-361a-465b-c9be-e8b8662b739a"},"source":["def tf_encode(en_t, sp_t):\n","    \"\"\"\n","    使用 tf.py_function 將 encode 函數轉換為 tensorflow 的輸入與輸出\n","    \"\"\"\n","    return tf.py_function(encode, [en_t, sp_t], [tf.int64, tf.int64])\n","\n","tmp_dataset = train_examples.map(tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","en_indices, sp_indices = next(iter(tmp_dataset))\n","\n","print('After tf_encode: (two tensor)')\n","print(en_indices)\n","print(sp_indices)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["After tf_encode: (two tensor)\n","tf.Tensor([8285   13   14   59  172 8075 8286], shape=(7,), dtype=int64)\n","tf.Tensor([8040   55 5647    2  153 7830 8041], shape=(7,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ds3x62Sv6WvY"},"source":["## 限制句子長度\n","\n","為了加快訓練速度，使用`tf.logical`限制句子長度，並使用`.filter`過濾"]},{"cell_type":"code","metadata":{"id":"c081xPGv1CPI"},"source":["max_length = 50\n","def filter_max_length(en_t, sp_t, max_length = max_length):\n","    \n","    return tf.logical_and(tf.size(en_t) <= max_length,\n","                          tf.size(sp_t) <= max_length)\n","\n","tmp_dataset = tmp_dataset.filter(filter_max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9i3ayTB6Wvb"},"source":["## Padding\n","\n","指定 `batch_size` 以及 `padding`，`padding` 會先檢查一個 `batch` 裡面的句子長度，不足最大長度的句子會補 `0` (`padding index`)，因為預設是補 `0` 的關係，所以字典中的  `word index` 必須從 `1` 開始計算，不然會跟 `padding index` 混淆。"]},{"cell_type":"code","metadata":{"id":"GtU484lD6Wvc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928222027,"user_tz":-480,"elapsed":9635,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"35db0948-3d55-4bd0-95e8-36b7080f7699"},"source":["batch_size = 64\n","tmp_dataset = tmp_dataset.padded_batch(batch_size=batch_size, padded_shapes=([-1], [-1]))\n","\n","en_batch, sp_batch = next(iter(tmp_dataset))\n","\n","print('英文 batch: ')\n","print(en_batch)\n","print('-' * 15)\n","print('西文 batch: ')\n","print(sp_batch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["英文 batch: \n","tf.Tensor(\n","[[8285   13   14 ...    0    0    0]\n"," [8285   56 2852 ...    0    0    0]\n"," [8285  134  153 ...    0    0    0]\n"," ...\n"," [8285    2 4507 ...    0    0    0]\n"," [8285    1   34 ...    0    0    0]\n"," [8285    1   49 ...    0    0    0]], shape=(64, 23), dtype=int64)\n","---------------\n","西文 batch: \n","tf.Tensor(\n","[[8040   55 5647 ...    0    0    0]\n"," [8040   20 4360 ...    0    0    0]\n"," [8040 5266 7816 ...    0    0    0]\n"," ...\n"," [8040    6 6523 ...    0    0    0]\n"," [8040   38 4508 ...    0    0    0]\n"," [8040   13  105 ...    0    0    0]], shape=(64, 23), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D1Ou_LXp6Wve"},"source":["# 將`train_examples`與`val_examples`做同樣處理\n","\n","* `train`:\n","\n"," - `map(tf_encode)`: 將字串轉成index\n"," - `filter(filter_max_length)`:過濾長度\n"," - `cache()`: cache the dataset to memory to get a speedup while reading from it.\n"," - `shuffle(buffer_size)`: 打亂buffer裡的資料，確保隨機\n"," - `padded_batch(batch_size, padded_shapes=([-1],[-1]))`: padding長度 \n","\n","Tensor-core pipeline: https://www.tensorflow.org/guide/performance/datasets?hl=zh_cn"]},{"cell_type":"code","metadata":{"id":"9mk9AZdZ5bcS"},"source":["max_length = 50\n","batch_size = 64\n","buffer_size = 15000\n","\n","train_dataset = (train_examples\n","                 .map(tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                 .filter(filter_max_length)\n","                 .cache()\n","                 .shuffle(buffer_size)\n","                 .padded_batch(batch_size, padded_shapes=([-1],[-1]))\n","                 .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","test_dataset = (test_examples\n","               .map(tf_encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","               .filter(filter_max_length)\n","               .padded_batch(batch_size, padded_shapes=([-1], [-1])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fXvfYVfQr2n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928227103,"user_tz":-480,"elapsed":14693,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"e643f535-5e26-4654-fcfd-acd7afa5a4c9"},"source":["%%time\n","en_batch, sp_batch = next(iter(train_dataset))\n","\n","print('英文 batch tensor: ')\n","print(en_batch)\n","print('-' * 20)\n","print('西文 batch tensor: ')\n","print(sp_batch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["英文 batch tensor: \n","tf.Tensor(\n","[[8285  202 1115 ...    0    0    0]\n"," [8285    1   18 ...    0    0    0]\n"," [8285  105  684 ...    0    0    0]\n"," ...\n"," [8285   55    4 ...    0    0    0]\n"," [8285    1   18 ...    0    0    0]\n"," [8285    1  294 ...    0    0    0]], shape=(64, 23), dtype=int64)\n","--------------------\n","西文 batch tensor: \n","tf.Tensor(\n","[[8040   14  851 ...    0    0    0]\n"," [8040   46    1 ...    0    0    0]\n"," [8040   15   71 ...    0    0    0]\n"," ...\n"," [8040    8   52 ...    0    0    0]\n"," [8040    5   46 ...    0    0    0]\n"," [8040  967    1 ...    0    0    0]], shape=(64, 22), dtype=int64)\n","CPU times: user 7.38 s, sys: 1.23 s, total: 8.61 s\n","Wall time: 5.33 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3IJA9SIK6Wvl"},"source":["# Seq2seq with attention mechanism\n"]},{"cell_type":"markdown","metadata":{"id":"HjYgKA4h6Wvm"},"source":["要理解Attention的想法，推薦閱讀:\n","\n","https://medium.com/ai-academy-taiwan/attention-mechanism-fad735db3c2c\n","\n","\n","以下程式碼分為三個主要部分:\n","\n","1. `Encoder`: 通常使用`RNN`系列的模型，例如`RNN`、`LSTM`、`GRU`等等，頂多再加上雙向的架構，所有的`RNN`模型在每個`timestep`都會產生一個`hidden state`，這個`hidden state`會準備給`Decoder`進行`attention`。\n","\n","2. `Attention layer`: 連接`Encoder`與`Decoder`之間的計算`attention`方式，這裡計算`attention`方式有很多種，基本上就是圍繞在兩個`hidden state`之間的attention計算方式，有`concatenation`、`fully connection`等等，之後產生`context vector`$c_t$。\n","\n","3. `Decoder`: 負責組合`Encoder`與`Decoder`的`hidden state`，這邊也有許多種組合方式"]},{"cell_type":"markdown","metadata":{"id":"szHKVijr6Wvm"},"source":["## 基本參數設置"]},{"cell_type":"code","metadata":{"id":"UQpvB32N6Wvm"},"source":["vocab_inp_size = tokenizer_en.vocab_size + 2\n","vocab_tar_size = tokenizer_sp.vocab_size + 2\n","\n","embedding_dim = 512\n","units = 1024\n","batch_size = 64\n","epochs = 20\n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfzIpOOP6Wvp"},"source":["## Encoder\n","\n","`Encoder`就是`RNN`，需要注意的是要將每個`timestep`都進行輸出，這裡我們使用`LSTM`。\n","\n","### LSTM parameters\n","\n"," - `return_sequences`：是否返回所有的`timestep`的`hidden_state`。\n"," - `return_state`：是否返回最後一個`timestep`的`cell_state`，注意只有`LSTM`有`cell_state`，其餘像`RNN,GRU`是沒有`cell_state`的。"]},{"cell_type":"code","metadata":{"id":"M8qDnamu6Wvq"},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_inp_size, embedding_dim, units, batch_size):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.units = units\n","        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_inp_size, output_dim=embedding_dim)\n","        self.lstm = tf.keras.layers.LSTM(units=units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","        \n","    def __call__(self, x):\n","        \"\"\"\n","        LSTM需要initial_state與initial_cell兩種初始值，這邊都使用同一個initial_hidden_state作為初始值\n","        若使用GRU則只需要一個initial_state即可\n","        \"\"\"\n","        x = self.embedding(x)\n","        states, last_state, cell_memory = self.lstm(x)\n","        return states, last_state\n","\n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_size, self.units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b2rw6iz6Wvt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928228250,"user_tz":-480,"elapsed":15817,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"83f94a41-046e-45a8-f2ee-3d2329d89c40"},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n","\n","initial_hidden = encoder.initialize_hidden_state()\n","states, last_state = encoder(en_batch)\n","print('Encoder hidden state for each timestep: ',states.shape) # (batch_size, sequence_length, units)\n","print('Encoder last hidden state: ',last_state.shape) # (batch_size, sequence_length, units)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoder hidden state for each timestep:  (64, 23, 1024)\n","Encoder last hidden state:  (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B8mBi1kw6Wvw"},"source":["## Attention mechanism\n","\n","這裡實作 `Bahdanau attention` 的架構，基本上就是圍繞在編碼器的 `hidden state` $\\bar{h}_s$ 與 解碼器的 `hidden state` $h_t$:\n","\n","1. [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, Bahdanau et al.](https://arxiv.org/pdf/1409.0473.pdf)\n","\n"," - 這篇論文使用 `fully connection` 來計算 $\\bar{h}_s$ 與 $h_t$ 的`attention`。\n"," \n"," \n","$$\n","\\Large{\\mathrm{score}(h_t,\\bar{h}_s)=v_a^T\\tanh(w_1h_t+w_2\\bar{h}_s)} \\\\\n","\\Large{v_a, h_t, \\bar{h}_s, w1,w2\\in \\{R^d, R^d, R^d, R^{d\\times d}, R^{d\\times d}\\}}\n","$$\n"," "]},{"cell_type":"markdown","metadata":{"id":"MstMeyNkkKnL"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1cH_2KjFglNNbY7oyXYq7DrRiOIPqt7aT' width=\"400\"/>\n","<figcaption>Attention mechanism</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"vcSfJqEWyWEU"},"source":["計算完每個位置的 `attention score` $A\\in R^N$ 之後，再將 `attention score` 與編碼器的 $\\bar{h}_s\\in R^{N\\times d}$ 做線性組合: "]},{"cell_type":"markdown","metadata":{"id":"Ss8dFx9e0hMS"},"source":["$$\n","C = \\sum_{i=1}^NA_i\\bar{h}_{s_i} \\in R^d\n","$$"]},{"cell_type":"code","metadata":{"id":"mkuVhuRO6Wvw"},"source":["class BahdanauAttention(tf.keras.Model):\n","    def __init__(self, units):\n","        super().__init__()\n","        self.w1 = tf.keras.layers.Dense(units)\n","        self.w2 = tf.keras.layers.Dense(units)\n","        self.v = tf.keras.layers.Dense(1)\n","    \n","    def __call__(self, query, keys):\n","        \"\"\"\n","        配合上圖，命名 query 和 keys 為了與 transformer 相互比較\n","        query: 為 decoder 的 hidden state, 就是 hs, (batch_size, units)\n","        keys: 為 encoder 每個 hidden state, 就是 ht, (batch_size, sequence_length, units)\n","        \"\"\"\n","        query = tf.expand_dims(query, axis=1)\n","        \"\"\"\n","        Bahdanau Attention\n","        輸出為(batch_size, sequence_length, 1), encoder 每個 ht 都有一個 score\n","        \"\"\"\n","        score = self.v(tf.nn.tanh(self.w1(keys) + self.w2(query)))\n","        \n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        \"\"\"\n","        在這邊就是將 attention_weights 與 keys 做線性組合得到 context vector c\n","        \"\"\"\n","        context_vector = tf.reduce_sum(attention_weights * keys, axis=1)\n","        \n","        return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuNc5_c46Wvy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928228252,"user_tz":-480,"elapsed":15801,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"b2356126-d4a3-455a-d561-3dd24d7fcaae"},"source":["attention_layer = BahdanauAttention(10)\n","context_vector, attention_weights = attention_layer(last_state, states)\n","print('context_vector shape: ', context_vector.shape)\n","print('attention_weights shape: ', attention_weights.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["context_vector shape:  (64, 1024)\n","attention_weights shape:  (64, 23, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LG0uIc0-lzJ_"},"source":["還有其他計算 `attention score` 方式也陸續提出來，如下圖: \n","\n","* `dot`: 向量內積\n","* `general`: 中間放一個權重 $W_a$。\n","* `concat`: 將兩個向量 `concat` ($h=[h_t;\\bar{h}_s]\\in R^{2d}$)，接著乘以權重 $W_a\\in R^{d\\times 2d}$，然後通過 `tanh` ($\\in R^{d}$)，在乘以權重 $v_a\\in R^{d}$ 得到 `attention score` ($score(h_t,\\bar{h}_s)={v_a}^\\top h\\in R$)。"]},{"cell_type":"markdown","metadata":{"id":"0vi-ipUJl47P"},"source":["<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1lRi8VxEVN9iBtVUOmmrtUs4KWyhj2OvQ' width=\"600\"/>\n","<figcaption>Attention mechanism</figcaption></center>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"-MGuSPmjni11"},"source":["再後來有許多的論文都是以向量內積 `dot` 為主，因為內積不會增加任何參數量，而且表現也不錯，目前在自然語言處理領域中最強的架構 `Transformer` 中，就是使用內積來計算兩個詞向量的相似度。"]},{"cell_type":"code","metadata":{"id":"xiy9-iHgrQX2"},"source":["class Attention(tf.keras.Model):\n","    def __init__(self, units, score_function = 'dot'):\n","        super().__init__()\n","        self.wa1 = tf.keras.layers.Dense(units) # for general\n","        self.wa2 = tf.keras.layers.Dense(units) # for concat\n","        self.va = tf.keras.layers.Dense(1) # for concat\n","        self.score_function = score_function\n","    def __call__(self, query, keys):\n","\n","        # query shape == (batch, 1, hidden_size)\n","        query = tf.expand_dims(query, axis=1)\n","\n","        if self.score_function == 'dot':\n","          score = tf.einsum('ijk,imk->ij', keys, query)\n","        elif self.score_function == 'general':\n","          score = tf.einsum('ijk,imk->ij', keys, self.wa1(query))\n","        elif self.score_function == 'concat':\n","          query = tf.broadcast_to(query, (query.shape[0], keys.shape[1], query.shape[2]))\n","          score = self.va(tf.nn.tanh(self.wa2(tf.concat([keys, query], axis=-1))))\n","          score = tf.squeeze(score, axis=2)\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        attention_weights = tf.expand_dims(attention_weights, axis=2)\n","        context_vector = tf.reduce_sum(attention_weights * keys, axis=1)\n","        \n","        return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"03otdSfY2Nbx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928228253,"user_tz":-480,"elapsed":15785,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"f2cbb130-04c7-449c-8b1f-1cd1fdff3fd9"},"source":["attention_layer = Attention(units, score_function = 'concat')\n","context_vector, attention_weights = attention_layer(last_state, states)\n","print('context_vector shape: ', context_vector.shape)\n","print('attention_weights shape: ', attention_weights.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["context_vector shape:  (64, 1024)\n","attention_weights shape:  (64, 23, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gV0nfSyR6Wv5"},"source":["## Decoder for Bahdanau\n","\n","透過`Attention`得到`context vector`之後，`Decoder`的作用就是決定如何串接`Encoder`的`hidden state`以及進行輸出預測。\n","\n","在 `Bahdanau attention` 中，是先把 `context vector` 與 `word embedding` 進行 `concat`，然後再通過 `lstm` 得到預測值。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9NnEA6FY36Ff"},"source":["$$\n","\\Large{\\tilde{h}=lstm([C;x]),\\;C,x,\\tilde{h}\\in \\{R^d,R^d,R^d\\}} \\\\\n","\\Large{\\hat{y}=Dense(\\tilde{h})\\in R^{vocab\\;size}}\n","$$"]},{"cell_type":"code","metadata":{"id":"cWwczBd06Wv5"},"source":["class Decoder_Bahdanau(tf.keras.Model):\n","  def __init__(self, vocab_tar_size, embedding_dim, dec_units):\n","    super().__init__()\n","    # 同上式中的 x\n","    self.embedding = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim)\n","    # 同上式中的 lstm\n","    self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    # 同上式中的 Dense\n","    self.fc = tf.keras.layers.Dense(vocab_tar_size)\n","\n","    # 同上式中計算 attention score C 的函數\n","    self.attention = BahdanauAttention(dec_units)\n","\n","  def __call__(self, x, hidden, enc_output, score_function = None):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    # hidden shape == (batch_size, hidden_size)\n","    # context vector shape == (batch_size, hidden_size)\n","    # attention_weights shape == (batch_size, inp_len, 1)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the LSTM\n","    # output shape == (batch_size, 1, dec_units)\n","    # state shape == (batch_size, dec_units)\n","    # cell_memory shape == (batch_size, dec_units)\n","    output, state, cell_memory = self.lstm(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.squeeze(output, axis=1)\n","\n","    # output shape == (batch_size, vocab_size)\n","    logits = self.fc(output)\n","\n","    return logits, state, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWgfNCJ46Wv_"},"source":["### Decoder for Bahdanau attention"]},{"cell_type":"code","metadata":{"id":"ct0AL_tf6WwA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928229094,"user_tz":-480,"elapsed":16607,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"730590c8-40bb-4899-d30b-0146a4bb984d"},"source":["decoder_Bahdanau = Decoder_Bahdanau(vocab_tar_size, embedding_dim, units)\n","logits, state, attention_weights = decoder_Bahdanau(sp_batch[:,-1:], initial_hidden, states)\n","\n","print('logits shape: ', logits.shape) # (batch_size, vocab_tar_size)\n","print('last_state shape: ', state.shape) # (batch_size, dec_units)\n","print('attention_weights shape: ', attention_weights.shape) # (batch_size, sequence_length, 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logits shape:  (64, 8042)\n","last_state shape:  (64, 1024)\n","attention_weights shape:  (64, 23, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z0GY0qB_RAMH"},"source":["## Decoder for other\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JPx6ZkwDv4gf"},"source":["這邊的作法與上面 `Decoder for other` 類似，差別只在於計算 `attention score` 的方式"]},{"cell_type":"code","metadata":{"id":"JDmEdB8gRDfT"},"source":["class Decoder_other(tf.keras.Model):\n","  def __init__(self, vocab_tar_size, embedding_dim, dec_units, score_function):\n","    super().__init__()\n","    # 同上式中的 x\n","    self.embedding = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim)\n","    # 同上式中的 lstm\n","    self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    # 同上式中的 Dense\n","    self.fc = tf.keras.layers.Dense(vocab_tar_size)\n","\n","    # 同上式中計算 attention score C 的函數\n","    self.attention = Attention(dec_units, score_function)\n","\n","  def __call__(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    # hidden shape == (batch_size, hidden_size)\n","    # context vector shape == (batch_size, hidden_size)\n","    # attention_weights shape == (batch_size, inp_len, 1)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the LSTM\n","    # output shape == (batch_size, 1, dec_units)\n","    # state shape == (batch_size, dec_units)\n","    # cell_memory shape == (batch_size, dec_units)\n","    output, state, cell_memory = self.lstm(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.squeeze(output, axis=1)\n","\n","    # output shape == (batch_size, vocab_size)\n","    logits = self.fc(output)\n","\n","    return logits, state, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2HaEkehK2vGr"},"source":["### Decoder for other attention"]},{"cell_type":"code","metadata":{"id":"LTnIOaVN2xQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928229103,"user_tz":-480,"elapsed":16598,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"b6784781-46e1-49fc-a11b-579655edc003"},"source":["decoder_other = Decoder_other(vocab_tar_size, embedding_dim, units, score_function = 'dot')\n","logits, state, attention_weights = decoder_other(sp_batch[:,-1:], initial_hidden, states)\n","\n","print('logits shape: ', logits.shape) # (batch_size, vocab_tar_size)\n","print('last_state shape: ', state.shape) # (batch_size, dec_units)\n","print('attention_weights shape: ', attention_weights.shape) # (batch_size, sequence_length, 1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["logits shape:  (64, 8042)\n","last_state shape:  (64, 1024)\n","attention_weights shape:  (64, 23, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rClkkSCc6WwE"},"source":["## Loss and metrics\n","\n","這裏定義損失函數，當 `Decoder` 預測出每個位置的詞時需要計算損失，這邊使用分類任務的損失函數 `CategoricalCrossentropy`。\n","\n","部分句子為因為 `padding` 而有許多的 `0`，但是那並不是真實的標籤，所以必須忽略 `padding` 位置的損失。\n"]},{"cell_type":"code","metadata":{"id":"yy4i6jGQ6WwE"},"source":["def loss_function(real, pred):\n","    \"\"\"\n","    Input:\n","    real: (batch_size, 1)\n","    pred: (batch_size, vocab_tar_size)\n","    \n","    Return: \n","    mean loss for current batch\n","    \n","    mask: (batch_size, 1)\n","    因為一個batch中有些句子會提早出現padding(index為0)，不需要計算 padding 的 loss，這裏使用 mask 來記錄 padding 的 index\n","    \"\"\"\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    \n","    \"\"\"\n","    from_logits: y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution.\n","    reduction: the reduction schedule of output loss vectors. `https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/Reduction`\n","    \"\"\"\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","    \n","    \"\"\"\n","    loss_: (batch_size, 1)\n","    \"\"\"\n","    loss_ = loss_object(real, pred)\n","    \n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask # 只計算非padding的loss\n","    \n","    return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIaHi5DO6WwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604928229118,"user_tz":-480,"elapsed":16598,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"83b94bc4-4bb7-4b5e-d35c-fe12f0414ffa"},"source":["real = tf.constant([[0.],[1.]], dtype=tf.float32)\n","pred = tf.constant([[0.3,0.2,0.5],[0,1,0]], dtype=tf.float32)\n","\n","mean_loss = loss_function(real, pred)\n","print('mean loss for current batch: ', mean_loss.numpy())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean loss for current batch:  0.42281893\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"coTS7ueS6WwK"},"source":["## Optimizer\n","\n","優化器通常會優先考慮 `Adam`。\n","\n","近期亦有許多表現不錯的優化器被研究出來，例如 `RangerLars` 在圖像任務上表現上就比 `Adam` 還要好。\n","\n","優化器表現可以參考: https://github.com/mgrankin/over9000"]},{"cell_type":"code","metadata":{"id":"EoZDHI-i6WwL"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mOzx3Lze4Dfu"},"source":["# radam = tfa.optimizers.RectifiedAdam()\n","# # ranger\n","# optimizer = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hj-5-skp6WwM"},"source":["## Checkpoints\n","\n","這裏定義儲存模型的方式:\n","\n","* `checkpoint_path`: 模型儲存路徑\n","* `ckpt`: 模型中的架構\n","* `ckpt_manager`: 模型儲存的策略，包含了架構 (`ckpt`), 路徑 (`checkpoint_path`), 儲存最近幾次 (`max_to_keep`)。"]},{"cell_type":"code","metadata":{"id":"Iy5sFP236WwN"},"source":["# score_function = 'general'\n","model_name = 'checkpoints_seq2seq_with_Bah_attention'\n","\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n","decoder_Bahdanau = Decoder_Bahdanau(vocab_tar_size, embedding_dim, units)\n","# decoder_concat = Decoder_other(vocab_tar_size, embedding_dim, units, score_function)\n","\n","checkpoint_path = os.path.join(output_dir, model_name)\n","ckpt = tf.train.Checkpoint(encoder = encoder, decoder = decoder_Bahdanau, optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jPUc-Ky6WwP"},"source":["## Train step\n","\n","這邊設定輸入一個英文句子進行訓練的方式，大致上的步驟為:\n","\n","1. 輸入句子給 `encoder` 獲得 `hidden state`，準備給 `decoder` 計算 `attention`。\n","2. 定義初始輸入 `<BOS>`。\n","\n","**以下進入迴圈**\n","\n","3. 使用 `<BOS>` 輸入給 `decoder` 獲得一個預測詞。\n","4. 預測詞與真實詞計算損失。\n","5. 使用 `tearch forcing` 策略，將真實詞丟回給模型繼續預測，而不是把預測詞丟回給模型。\n","\n","**以上為迴圈**\n","\n","6. 拿出模型所有參數以及損失，使用 `optimizer` 進行梯度下降。\n"]},{"cell_type":"code","metadata":{"id":"lcVdCO_E6WwP"},"source":["def train_step(inp, tar, decoder):\n","\n","    loss = 0\n","    \"\"\"\n","    使用 with tf.GradientTape() 來告訴 tensorflow 以下的執行過程都會涉及到梯度的更新\n","    \"\"\"\n","    with tf.GradientTape() as tape:\n","        # 首先產生 encoder 所有的 hidden states\n","        states, last_state = encoder(inp)\n","        \n","        # 使用 encoder 的最後一步 last_state 作為 decoder 的 initial state\n","        hidden_state = last_state\n","        \n","        # 因為一開始還沒有正確答案，所以必須輸入給 decoder 一個起始的 token\n","        # tokenizer_sp.vocab_size: <BOS> token\n","        dec_input = tf.expand_dims([tokenizer_sp.vocab_size] * batch_size, axis=1)\n","        \n","        # auto-regressive 迴圈，每次預測出一個詞\n","        for t in range(1, tar.shape[1]):\n","            # decoder 進行預測 \n","            predictions, hidden_state, _ = decoder(dec_input, hidden_state, states)\n","\n","            # 預測結果和標籤計算損失\n","            loss += loss_function(tar[:, t], predictions)\n","            \n","            # 使用 teacher forcing 策略，每次輸入給模型真實答案，而不是預測值，這樣做的原因是防止模型一直錯下去\n","            dec_input = tf.expand_dims(tar[:, t], axis=1)\n","    # 計算一個 batch 中的\b\b\b平均損失\n","    batch_loss = (loss / int(tar.shape[1]))\n","\n","    # 拿出模型所有參數\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    # 計算模型參數的 gradient\n","    gradients = tape.gradient(loss, variables)\n","\n","    # Pre-normalized gradient\n","    # (gradients, _) = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n","\n","    # 使用 gradient 進行梯度下降\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kmi6wO7AV2JS"},"source":["## Evaluation step\n","\n","\b定義評估方式，\u001d輸入為英文句子，輸出為預測西文。\n","\n","基本上與 `train_step` 雷同，差別在於兩個：\n","\n","1. 沒有進行梯度下降\n","2. 當模型預測出 `<EOS>` 時，即停止預測。"]},{"cell_type":"code","metadata":{"id":"WAyV_z8S6Wwa"},"source":["def evaluate(inp_sentence, decoder):\n","    \n","    # 輸入英文句子進行文字前處理\n","    inp_sentence = preprocess_sentence(inp_sentence)\n","\n","    # 英文句子轉換為 token\n","    inp_tokenized = tokenizer_en.encode(inp_sentence)\n","\n","    # 在英文句子前後加上 <BOS> 以及 <EOS>\n","    inp_id = [tokenizer_en.vocab_size] + inp_tokenized + [tokenizer_en.vocab_size+1]\n","\n","    # 新增一個 batch_size 維度，符合模型輸入\n","    inp_id = tf.expand_dims(inp_id, axis=0)\n","    \n","    # 定義 decoder 輸入 <BOS>\n","    dec_inp = tf.expand_dims([tokenizer_sp.vocab_size] * inp_id.shape[0], axis=0)\n","    \n","    # 產生 encoder 所有的 hidden states\n","    states, last_state = encoder(inp_id)\n","    hidden_state = last_state\n","    \n","    # 紀錄預測結果\n","    preds_ids = list()\n","        \n","    # 紀錄每次預測值的注意力矩陣\n","    attn_plot = np.zeros((max_length, len(inp_tokenized) + 2))\n","    \n","    # 此迴圈開始進行預測，直到 max_length 或是預測出 <EOS> 就停止\n","    for t in range(max_length):\n","        # decoder 進行預測\n","        preds, hidden_state, attention_weights = decoder(dec_inp, hidden_state, states)\n","\n","        # 使用 argmax 挑出預測概率最高的 token\n","        preds_id = tf.argmax(preds[0], axis=0).numpy()\n","        \n","        # 新增一個 `batch_size` 維度，符合模型輸入\n","        dec_inp = tf.expand_dims([preds_id], axis=0)\n","        \n","        # 紀錄預測結果\n","        preds_ids.append(preds_id)\n","\n","        # attention_weights 的維度為 (1, predicted_seq_len, 1)，使用 tf.squeeze 將維度為 1 拿掉\n","        attention_weights = tf.squeeze(attention_weights).numpy()\n","        \n","        # 紀錄注意力矩陣\n","        attn_plot[t] = attention_weights\n","\n","        # 如果預測出 <BOS> 就跳出此迴圈\n","        if preds_id == tokenizer_sp.vocab_size + 1:\n","          attn_plot = attn_plot[:t+1, :]\n","          break\n","          \n","\n","    # 去除非 <BOS> 和 <EOS> 的 token\n","    preds_ids = [ids for ids in preds_ids if ids < tokenizer_sp.vocab_size]\n","    \n","    # 將 token 還原成字串\n","    preds_sent = tokenizer_sp.decode(preds_ids)\n","\n","    return preds_sent, inp_sentence, attn_plot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcsXoGFb6WwY"},"source":["## Training\n","\n","開始訓練過程"]},{"cell_type":"code","metadata":{"id":"j22Sp0bZ6WwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604945031841,"user_tz":-480,"elapsed":2230554,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"84f935f2-fa50-4c89-e709-1edadfe1cdca"},"source":["for epoch in tqdm.tqdm(tf.range(epochs)):\n","    start = time.time()\n","    \n","    total_loss = 0\n","    \n","    for (batch, (inp, tar)) in enumerate(train_dataset):\n","        # 如果輸入的句子數量不滿一個 `batch_size`，就複製最後一個句子直到達到 `batch_size`\n","        if inp.shape[0] != batch_size:\n","            repeats = batch_size - inp.shape[0]\n","            \n","            rep_inp = tf.convert_to_tensor(np.repeat(inp[-1:,:], repeats=repeats, axis=0))\n","            inp = tf.concat([inp, rep_inp], axis=0)\n","            \n","            rep_tar = tf.convert_to_tensor(np.repeat(tar[-1:,:], repeats=repeats, axis=0))\n","            tar = tf.concat([tar,rep_tar], axis=0)\n","            \n","        batch_loss = train_step(inp, tar, decoder_Bahdanau)\n","        \n","        total_loss += batch_loss\n","        \n","        if batch % 50 == 0:\n","            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n","            \n","    if (epoch + 1) % 2 == 0:\n","        print('Save Model!')\n","        ckpt_manager.save()\n","\n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / (batch+1)))\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","    result, sentence, attention_plot = evaluate('I am hungry.', decoder_Bahdanau)\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: %s' % (result))\n","    print('-'*20)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/20 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 4.7441\n","Epoch 1 Batch 50 Loss 3.3628\n","Epoch 1 Batch 100 Loss 1.7734\n","Epoch 1 Batch 150 Loss 3.0198\n","Epoch 1 Batch 200 Loss 2.7325\n","Epoch 1 Batch 250 Loss 2.6785\n","Epoch 1 Batch 300 Loss 1.2412\n","Epoch 1 Batch 350 Loss 1.8202\n","Epoch 1 Batch 400 Loss 2.2067\n","Epoch 1 Batch 450 Loss 2.2708\n","Epoch 1 Batch 500 Loss 1.9765\n","Epoch 1 Batch 550 Loss 2.4701\n","Epoch 1 Batch 600 Loss 2.5842\n","Epoch 1 Batch 650 Loss 1.9155\n","Epoch 1 Batch 700 Loss 2.7559\n","Epoch 1 Batch 750 Loss 2.0711\n","Epoch 1 Batch 800 Loss 1.9349\n","Epoch 1 Batch 850 Loss 2.1684\n","Epoch 1 Batch 900 Loss 2.1855\n","Epoch 1 Batch 950 Loss 2.0649\n","Epoch 1 Batch 1000 Loss 2.3459\n","Epoch 1 Batch 1050 Loss 1.7917\n","Epoch 1 Batch 1100 Loss 2.0473\n","Epoch 1 Batch 1150 Loss 2.0277\n","Epoch 1 Batch 1200 Loss 2.2901\n","Epoch 1 Batch 1250 Loss 1.7167\n","Epoch 1 Batch 1300 Loss 2.2421\n","Epoch 1 Batch 1350 Loss 1.8546\n","Epoch 1 Batch 1400 Loss 1.8268\n","Epoch 1 Batch 1450 Loss 2.2387\n","Epoch 1 Batch 1500 Loss 1.3256\n","Epoch 1 Batch 1550 Loss 2.2229\n","Epoch 1 Batch 1600 Loss 2.1061\n","Epoch 1 Batch 1650 Loss 1.4322\n"],"name":"stdout"},{"output_type":"stream","text":["\r  5%|▌         | 1/20 [14:01<4:26:37, 841.95s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1 Loss 2.1641\n","Time taken for 1 epoch 841.9200496673584 sec\n","\n","Input: i am hungry.\n","Predicted translation: el trabajo.\n","--------------------\n","Epoch 2 Batch 0 Loss 1.5350\n","Epoch 2 Batch 50 Loss 1.9693\n","Epoch 2 Batch 100 Loss 1.6483\n","Epoch 2 Batch 150 Loss 1.9267\n","Epoch 2 Batch 200 Loss 1.6363\n","Epoch 2 Batch 250 Loss 1.6256\n","Epoch 2 Batch 300 Loss 1.5858\n","Epoch 2 Batch 350 Loss 1.5216\n","Epoch 2 Batch 400 Loss 1.7056\n","Epoch 2 Batch 450 Loss 1.3708\n","Epoch 2 Batch 500 Loss 1.6856\n","Epoch 2 Batch 550 Loss 1.4339\n","Epoch 2 Batch 600 Loss 2.0021\n","Epoch 2 Batch 650 Loss 1.4917\n","Epoch 2 Batch 700 Loss 1.2657\n","Epoch 2 Batch 750 Loss 1.8400\n","Epoch 2 Batch 800 Loss 1.6991\n","Epoch 2 Batch 850 Loss 1.4902\n","Epoch 2 Batch 900 Loss 1.8422\n","Epoch 2 Batch 950 Loss 1.6912\n","Epoch 2 Batch 1000 Loss 1.6035\n","Epoch 2 Batch 1050 Loss 1.2278\n","Epoch 2 Batch 1100 Loss 1.2901\n","Epoch 2 Batch 1150 Loss 1.4783\n","Epoch 2 Batch 1200 Loss 1.3181\n","Epoch 2 Batch 1250 Loss 1.5265\n","Epoch 2 Batch 1300 Loss 1.5634\n","Epoch 2 Batch 1350 Loss 1.7758\n","Epoch 2 Batch 1400 Loss 1.0972\n","Epoch 2 Batch 1450 Loss 1.2232\n","Epoch 2 Batch 1500 Loss 1.4141\n","Epoch 2 Batch 1550 Loss 0.9257\n","Epoch 2 Batch 1600 Loss 1.4838\n","Epoch 2 Batch 1650 Loss 1.4313\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 10%|█         | 2/20 [27:54<4:11:43, 839.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2 Loss 1.5420\n","Time taken for 1 epoch 832.333865404129 sec\n","\n","Input: i am hungry.\n","Predicted translation: yo me gusta.\n","--------------------\n","Epoch 3 Batch 0 Loss 1.3105\n","Epoch 3 Batch 50 Loss 1.3492\n","Epoch 3 Batch 100 Loss 1.4503\n","Epoch 3 Batch 150 Loss 1.2025\n","Epoch 3 Batch 200 Loss 1.3244\n","Epoch 3 Batch 250 Loss 1.1670\n","Epoch 3 Batch 300 Loss 1.3638\n","Epoch 3 Batch 350 Loss 1.3452\n","Epoch 3 Batch 400 Loss 1.0878\n","Epoch 3 Batch 450 Loss 0.9420\n","Epoch 3 Batch 500 Loss 1.0893\n","Epoch 3 Batch 550 Loss 0.9765\n","Epoch 3 Batch 600 Loss 1.2737\n","Epoch 3 Batch 650 Loss 0.6153\n","Epoch 3 Batch 700 Loss 0.9338\n","Epoch 3 Batch 750 Loss 0.9224\n","Epoch 3 Batch 800 Loss 1.0526\n","Epoch 3 Batch 850 Loss 1.0976\n","Epoch 3 Batch 900 Loss 1.3657\n","Epoch 3 Batch 950 Loss 1.0830\n","Epoch 3 Batch 1000 Loss 1.2769\n","Epoch 3 Batch 1050 Loss 1.1162\n","Epoch 3 Batch 1100 Loss 1.2388\n","Epoch 3 Batch 1150 Loss 0.9944\n","Epoch 3 Batch 1200 Loss 1.2259\n","Epoch 3 Batch 1250 Loss 0.7456\n","Epoch 3 Batch 1300 Loss 1.3174\n","Epoch 3 Batch 1350 Loss 1.3024\n","Epoch 3 Batch 1400 Loss 1.1943\n","Epoch 3 Batch 1450 Loss 1.1881\n","Epoch 3 Batch 1500 Loss 1.1494\n","Epoch 3 Batch 1550 Loss 1.1892\n","Epoch 3 Batch 1600 Loss 1.2185\n","Epoch 3 Batch 1650 Loss 0.9893\n"],"name":"stdout"},{"output_type":"stream","text":["\r 15%|█▌        | 3/20 [42:23<4:00:16, 848.05s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3 Loss 1.2383\n","Time taken for 1 epoch 868.9343054294586 sec\n","\n","Input: i am hungry.\n","Predicted translation: yo tengo hambre.\n","--------------------\n","Epoch 4 Batch 0 Loss 1.1423\n","Epoch 4 Batch 50 Loss 1.1507\n","Epoch 4 Batch 100 Loss 1.3275\n","Epoch 4 Batch 150 Loss 1.0912\n","Epoch 4 Batch 200 Loss 0.9543\n","Epoch 4 Batch 250 Loss 1.1174\n","Epoch 4 Batch 300 Loss 1.3558\n","Epoch 4 Batch 350 Loss 1.1243\n","Epoch 4 Batch 400 Loss 1.1566\n","Epoch 4 Batch 450 Loss 1.1133\n","Epoch 4 Batch 500 Loss 0.9434\n","Epoch 4 Batch 550 Loss 1.1023\n","Epoch 4 Batch 600 Loss 1.2379\n","Epoch 4 Batch 650 Loss 0.9681\n","Epoch 4 Batch 700 Loss 0.9749\n","Epoch 4 Batch 750 Loss 1.0369\n","Epoch 4 Batch 800 Loss 1.0081\n","Epoch 4 Batch 850 Loss 0.7666\n","Epoch 4 Batch 900 Loss 0.7331\n","Epoch 4 Batch 950 Loss 0.7533\n","Epoch 4 Batch 1000 Loss 0.7551\n","Epoch 4 Batch 1050 Loss 0.9196\n","Epoch 4 Batch 1100 Loss 0.8241\n","Epoch 4 Batch 1150 Loss 0.9079\n","Epoch 4 Batch 1200 Loss 1.0138\n","Epoch 4 Batch 1250 Loss 0.9774\n","Epoch 4 Batch 1300 Loss 0.8068\n","Epoch 4 Batch 1350 Loss 1.0114\n","Epoch 4 Batch 1400 Loss 1.0228\n","Epoch 4 Batch 1450 Loss 1.1788\n","Epoch 4 Batch 1500 Loss 0.8008\n","Epoch 4 Batch 1550 Loss 1.1324\n","Epoch 4 Batch 1600 Loss 0.6598\n","Epoch 4 Batch 1650 Loss 0.8848\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 20%|██        | 4/20 [56:56<3:48:08, 855.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 4 Loss 1.0217\n","Time taken for 1 epoch 873.0477342605591 sec\n","\n","Input: i am hungry.\n","Predicted translation: \n","--------------------\n","Epoch 5 Batch 0 Loss 0.6686\n","Epoch 5 Batch 50 Loss 1.2373\n","Epoch 5 Batch 100 Loss 0.9820\n","Epoch 5 Batch 150 Loss 0.8753\n","Epoch 5 Batch 200 Loss 0.7323\n","Epoch 5 Batch 250 Loss 0.8457\n","Epoch 5 Batch 300 Loss 0.9094\n","Epoch 5 Batch 350 Loss 0.9550\n","Epoch 5 Batch 400 Loss 0.7145\n","Epoch 5 Batch 450 Loss 0.6454\n","Epoch 5 Batch 500 Loss 0.6815\n","Epoch 5 Batch 550 Loss 0.7755\n","Epoch 5 Batch 600 Loss 0.8278\n","Epoch 5 Batch 650 Loss 0.8903\n","Epoch 5 Batch 700 Loss 0.7700\n","Epoch 5 Batch 750 Loss 0.9878\n","Epoch 5 Batch 800 Loss 0.8537\n","Epoch 5 Batch 850 Loss 1.1014\n","Epoch 5 Batch 900 Loss 0.5695\n","Epoch 5 Batch 950 Loss 0.9733\n","Epoch 5 Batch 1000 Loss 0.6441\n","Epoch 5 Batch 1050 Loss 0.8283\n","Epoch 5 Batch 1100 Loss 0.8494\n","Epoch 5 Batch 1150 Loss 0.7057\n","Epoch 5 Batch 1200 Loss 0.8357\n","Epoch 5 Batch 1250 Loss 0.7523\n","Epoch 5 Batch 1300 Loss 0.5809\n","Epoch 5 Batch 1350 Loss 0.9643\n","Epoch 5 Batch 1400 Loss 0.8657\n","Epoch 5 Batch 1450 Loss 0.7503\n","Epoch 5 Batch 1500 Loss 0.7029\n","Epoch 5 Batch 1550 Loss 0.9955\n","Epoch 5 Batch 1600 Loss 0.7431\n","Epoch 5 Batch 1650 Loss 0.9156\n"],"name":"stdout"},{"output_type":"stream","text":["\r 25%|██▌       | 5/20 [1:11:31<3:35:20, 861.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 5 Loss 0.8631\n","Time taken for 1 epoch 874.9329748153687 sec\n","\n","Input: i am hungry.\n","Predicted translation: yo almorgre.\n","--------------------\n","Epoch 6 Batch 0 Loss 0.8127\n","Epoch 6 Batch 50 Loss 0.7125\n","Epoch 6 Batch 100 Loss 0.5591\n","Epoch 6 Batch 150 Loss 0.6523\n","Epoch 6 Batch 200 Loss 0.7366\n","Epoch 6 Batch 250 Loss 0.8859\n","Epoch 6 Batch 300 Loss 0.5649\n","Epoch 6 Batch 350 Loss 0.8791\n","Epoch 6 Batch 400 Loss 0.6826\n","Epoch 6 Batch 450 Loss 0.4820\n","Epoch 6 Batch 500 Loss 0.6892\n","Epoch 6 Batch 550 Loss 0.8404\n","Epoch 6 Batch 600 Loss 0.7758\n","Epoch 6 Batch 650 Loss 0.9210\n","Epoch 6 Batch 700 Loss 0.8253\n","Epoch 6 Batch 750 Loss 0.9925\n","Epoch 6 Batch 800 Loss 0.7112\n","Epoch 6 Batch 850 Loss 0.6810\n","Epoch 6 Batch 900 Loss 0.6295\n","Epoch 6 Batch 950 Loss 0.8210\n","Epoch 6 Batch 1000 Loss 0.7833\n","Epoch 6 Batch 1050 Loss 0.7486\n","Epoch 6 Batch 1100 Loss 0.4181\n","Epoch 6 Batch 1150 Loss 0.6546\n","Epoch 6 Batch 1200 Loss 0.6623\n","Epoch 6 Batch 1250 Loss 0.4918\n","Epoch 6 Batch 1300 Loss 0.6801\n","Epoch 6 Batch 1350 Loss 0.7406\n","Epoch 6 Batch 1400 Loss 0.6258\n","Epoch 6 Batch 1450 Loss 0.9086\n","Epoch 6 Batch 1500 Loss 0.6955\n","Epoch 6 Batch 1550 Loss 0.8813\n","Epoch 6 Batch 1600 Loss 0.6209\n","Epoch 6 Batch 1650 Loss 0.6203\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 30%|███       | 6/20 [1:26:05<3:21:51, 865.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 6 Loss 0.7341\n","Time taken for 1 epoch 873.8347284793854 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 7 Batch 0 Loss 0.9143\n","Epoch 7 Batch 50 Loss 0.5199\n","Epoch 7 Batch 100 Loss 0.7036\n","Epoch 7 Batch 150 Loss 0.6401\n","Epoch 7 Batch 200 Loss 0.7591\n","Epoch 7 Batch 250 Loss 0.4830\n","Epoch 7 Batch 300 Loss 0.5039\n","Epoch 7 Batch 350 Loss 0.6833\n","Epoch 7 Batch 400 Loss 0.6490\n","Epoch 7 Batch 450 Loss 0.8525\n","Epoch 7 Batch 500 Loss 0.6689\n","Epoch 7 Batch 550 Loss 0.9067\n","Epoch 7 Batch 600 Loss 0.6431\n","Epoch 7 Batch 650 Loss 0.5993\n","Epoch 7 Batch 700 Loss 0.7194\n","Epoch 7 Batch 750 Loss 0.5073\n","Epoch 7 Batch 800 Loss 0.5265\n","Epoch 7 Batch 850 Loss 0.8995\n","Epoch 7 Batch 900 Loss 0.5704\n","Epoch 7 Batch 950 Loss 0.7480\n","Epoch 7 Batch 1000 Loss 0.6757\n","Epoch 7 Batch 1050 Loss 0.5717\n","Epoch 7 Batch 1100 Loss 0.4419\n","Epoch 7 Batch 1150 Loss 0.4640\n","Epoch 7 Batch 1200 Loss 0.5568\n","Epoch 7 Batch 1250 Loss 0.6059\n","Epoch 7 Batch 1300 Loss 0.8494\n","Epoch 7 Batch 1350 Loss 0.6281\n","Epoch 7 Batch 1400 Loss 0.5632\n","Epoch 7 Batch 1450 Loss 0.7092\n","Epoch 7 Batch 1500 Loss 0.7182\n","Epoch 7 Batch 1550 Loss 0.6782\n","Epoch 7 Batch 1600 Loss 0.6582\n","Epoch 7 Batch 1650 Loss 0.5151\n"],"name":"stdout"},{"output_type":"stream","text":["\r 35%|███▌      | 7/20 [1:40:23<3:06:58, 862.93s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 7 Loss 0.6318\n","Time taken for 1 epoch 857.7802379131317 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 8 Batch 0 Loss 0.6218\n","Epoch 8 Batch 50 Loss 0.7546\n","Epoch 8 Batch 100 Loss 0.5321\n","Epoch 8 Batch 150 Loss 0.3523\n","Epoch 8 Batch 200 Loss 0.6694\n","Epoch 8 Batch 250 Loss 0.4840\n","Epoch 8 Batch 300 Loss 0.6411\n","Epoch 8 Batch 350 Loss 0.6856\n","Epoch 8 Batch 400 Loss 0.4672\n","Epoch 8 Batch 450 Loss 0.4956\n","Epoch 8 Batch 500 Loss 0.5534\n","Epoch 8 Batch 550 Loss 0.6055\n","Epoch 8 Batch 600 Loss 0.5880\n","Epoch 8 Batch 650 Loss 0.7014\n","Epoch 8 Batch 700 Loss 0.5125\n","Epoch 8 Batch 750 Loss 0.5099\n","Epoch 8 Batch 800 Loss 0.5575\n","Epoch 8 Batch 850 Loss 0.6046\n","Epoch 8 Batch 900 Loss 0.4202\n","Epoch 8 Batch 950 Loss 0.5058\n","Epoch 8 Batch 1000 Loss 0.5986\n","Epoch 8 Batch 1050 Loss 0.4104\n","Epoch 8 Batch 1100 Loss 0.5390\n","Epoch 8 Batch 1150 Loss 0.6453\n","Epoch 8 Batch 1200 Loss 0.3956\n","Epoch 8 Batch 1250 Loss 0.5634\n","Epoch 8 Batch 1300 Loss 0.5246\n","Epoch 8 Batch 1350 Loss 0.4886\n","Epoch 8 Batch 1400 Loss 0.4415\n","Epoch 8 Batch 1450 Loss 0.4067\n","Epoch 8 Batch 1500 Loss 0.4959\n","Epoch 8 Batch 1550 Loss 0.6578\n","Epoch 8 Batch 1600 Loss 0.5769\n","Epoch 8 Batch 1650 Loss 0.3677\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 40%|████      | 8/20 [1:54:39<2:52:11, 860.95s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 8 Loss 0.5417\n","Time taken for 1 epoch 856.2971820831299 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 9 Batch 0 Loss 0.5969\n","Epoch 9 Batch 50 Loss 0.5183\n","Epoch 9 Batch 100 Loss 0.5049\n","Epoch 9 Batch 150 Loss 0.5236\n","Epoch 9 Batch 200 Loss 0.6281\n","Epoch 9 Batch 250 Loss 0.5206\n","Epoch 9 Batch 300 Loss 0.5775\n","Epoch 9 Batch 350 Loss 0.5232\n","Epoch 9 Batch 400 Loss 0.4214\n","Epoch 9 Batch 450 Loss 0.5090\n","Epoch 9 Batch 500 Loss 0.6476\n","Epoch 9 Batch 550 Loss 0.6221\n","Epoch 9 Batch 600 Loss 0.4849\n","Epoch 9 Batch 650 Loss 0.4792\n","Epoch 9 Batch 700 Loss 0.5366\n","Epoch 9 Batch 750 Loss 0.4945\n","Epoch 9 Batch 800 Loss 0.4819\n","Epoch 9 Batch 850 Loss 0.4024\n","Epoch 9 Batch 900 Loss 0.4030\n","Epoch 9 Batch 950 Loss 0.4811\n","Epoch 9 Batch 1000 Loss 0.5069\n","Epoch 9 Batch 1050 Loss 0.6289\n","Epoch 9 Batch 1100 Loss 0.3969\n","Epoch 9 Batch 1150 Loss 0.4614\n","Epoch 9 Batch 1200 Loss 0.5311\n","Epoch 9 Batch 1250 Loss 0.4394\n","Epoch 9 Batch 1300 Loss 0.3487\n","Epoch 9 Batch 1350 Loss 0.3843\n","Epoch 9 Batch 1400 Loss 0.2888\n","Epoch 9 Batch 1450 Loss 0.3870\n","Epoch 9 Batch 1500 Loss 0.4335\n","Epoch 9 Batch 1550 Loss 0.4613\n","Epoch 9 Batch 1600 Loss 0.3819\n","Epoch 9 Batch 1650 Loss 0.3361\n"],"name":"stdout"},{"output_type":"stream","text":["\r 45%|████▌     | 9/20 [2:08:35<2:36:28, 853.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 9 Loss 0.4652\n","Time taken for 1 epoch 836.0119762420654 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 10 Batch 0 Loss 0.4921\n","Epoch 10 Batch 50 Loss 0.5361\n","Epoch 10 Batch 100 Loss 0.4723\n","Epoch 10 Batch 150 Loss 0.5217\n","Epoch 10 Batch 200 Loss 0.4558\n","Epoch 10 Batch 250 Loss 0.4535\n","Epoch 10 Batch 300 Loss 0.4333\n","Epoch 10 Batch 350 Loss 0.2603\n","Epoch 10 Batch 400 Loss 0.3810\n","Epoch 10 Batch 450 Loss 0.3967\n","Epoch 10 Batch 500 Loss 0.3792\n","Epoch 10 Batch 550 Loss 0.3846\n","Epoch 10 Batch 600 Loss 0.4099\n","Epoch 10 Batch 650 Loss 0.4396\n","Epoch 10 Batch 700 Loss 0.3780\n","Epoch 10 Batch 750 Loss 0.5053\n","Epoch 10 Batch 800 Loss 0.4778\n","Epoch 10 Batch 850 Loss 0.4065\n","Epoch 10 Batch 900 Loss 0.3865\n","Epoch 10 Batch 950 Loss 0.4242\n","Epoch 10 Batch 1000 Loss 0.3642\n","Epoch 10 Batch 1050 Loss 0.4068\n","Epoch 10 Batch 1100 Loss 0.4510\n","Epoch 10 Batch 1150 Loss 0.3572\n","Epoch 10 Batch 1200 Loss 0.3558\n","Epoch 10 Batch 1250 Loss 0.3508\n","Epoch 10 Batch 1300 Loss 0.2543\n","Epoch 10 Batch 1350 Loss 0.3212\n","Epoch 10 Batch 1400 Loss 0.3834\n","Epoch 10 Batch 1450 Loss 0.2194\n","Epoch 10 Batch 1500 Loss 0.4335\n","Epoch 10 Batch 1550 Loss 0.3612\n","Epoch 10 Batch 1600 Loss 0.4514\n","Epoch 10 Batch 1650 Loss 0.3468\n","Save Model!\n","Epoch 10 Loss 0.3960\n","Time taken for 1 epoch 843.9443230628967 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 10/20 [2:22:39<2:21:47, 850.72s/it]"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: me pegregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregre\n","--------------------\n","Epoch 11 Batch 0 Loss 0.3228\n","Epoch 11 Batch 50 Loss 0.1988\n","Epoch 11 Batch 100 Loss 0.4277\n","Epoch 11 Batch 150 Loss 0.3234\n","Epoch 11 Batch 200 Loss 0.4350\n","Epoch 11 Batch 250 Loss 0.4100\n","Epoch 11 Batch 300 Loss 0.4183\n","Epoch 11 Batch 350 Loss 0.3341\n","Epoch 11 Batch 400 Loss 0.3601\n","Epoch 11 Batch 450 Loss 0.3822\n","Epoch 11 Batch 500 Loss 0.3247\n","Epoch 11 Batch 550 Loss 0.4260\n","Epoch 11 Batch 600 Loss 0.3193\n","Epoch 11 Batch 650 Loss 0.3962\n","Epoch 11 Batch 700 Loss 0.3886\n","Epoch 11 Batch 750 Loss 0.3872\n","Epoch 11 Batch 800 Loss 0.4290\n","Epoch 11 Batch 850 Loss 0.3169\n","Epoch 11 Batch 900 Loss 0.3696\n","Epoch 11 Batch 950 Loss 0.2734\n","Epoch 11 Batch 1000 Loss 0.3197\n","Epoch 11 Batch 1050 Loss 0.3109\n","Epoch 11 Batch 1100 Loss 0.3374\n","Epoch 11 Batch 1150 Loss 0.4063\n","Epoch 11 Batch 1200 Loss 0.3228\n","Epoch 11 Batch 1250 Loss 0.2854\n","Epoch 11 Batch 1300 Loss 0.2598\n","Epoch 11 Batch 1350 Loss 0.3131\n","Epoch 11 Batch 1400 Loss 0.2656\n","Epoch 11 Batch 1450 Loss 0.4564\n","Epoch 11 Batch 1500 Loss 0.2596\n","Epoch 11 Batch 1550 Loss 0.2871\n","Epoch 11 Batch 1600 Loss 0.2842\n","Epoch 11 Batch 1650 Loss 0.2884\n"],"name":"stdout"},{"output_type":"stream","text":["\r 55%|█████▌    | 11/20 [2:36:51<2:07:40, 851.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 11 Loss 0.3379\n","Time taken for 1 epoch 852.1255362033844 sec\n","\n","Input: i am hungry.\n","Predicted translation: \n","--------------------\n","Epoch 12 Batch 0 Loss 0.3352\n","Epoch 12 Batch 50 Loss 0.4216\n","Epoch 12 Batch 100 Loss 0.3109\n","Epoch 12 Batch 150 Loss 0.3163\n","Epoch 12 Batch 200 Loss 0.2946\n","Epoch 12 Batch 250 Loss 0.2677\n","Epoch 12 Batch 300 Loss 0.3138\n","Epoch 12 Batch 350 Loss 0.2773\n","Epoch 12 Batch 400 Loss 0.2847\n","Epoch 12 Batch 450 Loss 0.3158\n","Epoch 12 Batch 500 Loss 0.2824\n","Epoch 12 Batch 550 Loss 0.3083\n","Epoch 12 Batch 600 Loss 0.2827\n","Epoch 12 Batch 650 Loss 0.3531\n","Epoch 12 Batch 700 Loss 0.2760\n","Epoch 12 Batch 750 Loss 0.2958\n","Epoch 12 Batch 800 Loss 0.1885\n","Epoch 12 Batch 850 Loss 0.3960\n","Epoch 12 Batch 900 Loss 0.3818\n","Epoch 12 Batch 950 Loss 0.2338\n","Epoch 12 Batch 1000 Loss 0.2738\n","Epoch 12 Batch 1050 Loss 0.2439\n","Epoch 12 Batch 1100 Loss 0.3465\n","Epoch 12 Batch 1150 Loss 0.2765\n","Epoch 12 Batch 1200 Loss 0.3045\n","Epoch 12 Batch 1250 Loss 0.2687\n","Epoch 12 Batch 1300 Loss 0.3035\n","Epoch 12 Batch 1350 Loss 0.2085\n","Epoch 12 Batch 1400 Loss 0.2732\n","Epoch 12 Batch 1450 Loss 0.2694\n","Epoch 12 Batch 1500 Loss 0.2632\n","Epoch 12 Batch 1550 Loss 0.2475\n","Epoch 12 Batch 1600 Loss 0.3059\n","Epoch 12 Batch 1650 Loss 0.2403\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 60%|██████    | 12/20 [2:50:51<1:53:01, 847.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 12 Loss 0.2869\n","Time taken for 1 epoch 839.4742028713226 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 13 Batch 0 Loss 0.3196\n","Epoch 13 Batch 50 Loss 0.2842\n","Epoch 13 Batch 100 Loss 0.1824\n","Epoch 13 Batch 150 Loss 0.2384\n","Epoch 13 Batch 200 Loss 0.2783\n","Epoch 13 Batch 250 Loss 0.1736\n","Epoch 13 Batch 300 Loss 0.2152\n","Epoch 13 Batch 350 Loss 0.1703\n","Epoch 13 Batch 400 Loss 0.2136\n","Epoch 13 Batch 450 Loss 0.1939\n","Epoch 13 Batch 500 Loss 0.2525\n","Epoch 13 Batch 550 Loss 0.2478\n","Epoch 13 Batch 600 Loss 0.2517\n","Epoch 13 Batch 650 Loss 0.2301\n","Epoch 13 Batch 700 Loss 0.2680\n","Epoch 13 Batch 750 Loss 0.2149\n","Epoch 13 Batch 800 Loss 0.3034\n","Epoch 13 Batch 850 Loss 0.2587\n","Epoch 13 Batch 900 Loss 0.2711\n","Epoch 13 Batch 950 Loss 0.2441\n","Epoch 13 Batch 1000 Loss 0.2735\n","Epoch 13 Batch 1050 Loss 0.2046\n","Epoch 13 Batch 1100 Loss 0.3092\n","Epoch 13 Batch 1150 Loss 0.2342\n","Epoch 13 Batch 1200 Loss 0.2914\n","Epoch 13 Batch 1250 Loss 0.2628\n","Epoch 13 Batch 1300 Loss 0.2330\n","Epoch 13 Batch 1350 Loss 0.1932\n","Epoch 13 Batch 1400 Loss 0.2075\n","Epoch 13 Batch 1450 Loss 0.2195\n","Epoch 13 Batch 1500 Loss 0.2057\n","Epoch 13 Batch 1550 Loss 0.2324\n","Epoch 13 Batch 1600 Loss 0.2408\n","Epoch 13 Batch 1650 Loss 0.1879\n"],"name":"stdout"},{"output_type":"stream","text":["\r 65%|██████▌   | 13/20 [3:04:19<1:37:30, 835.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 13 Loss 0.2413\n","Time taken for 1 epoch 808.261735200882 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 14 Batch 0 Loss 0.2043\n","Epoch 14 Batch 50 Loss 0.2160\n","Epoch 14 Batch 100 Loss 0.1795\n","Epoch 14 Batch 150 Loss 0.2120\n","Epoch 14 Batch 200 Loss 0.1607\n","Epoch 14 Batch 250 Loss 0.2397\n","Epoch 14 Batch 300 Loss 0.1986\n","Epoch 14 Batch 350 Loss 0.1837\n","Epoch 14 Batch 400 Loss 0.2168\n","Epoch 14 Batch 450 Loss 0.1589\n","Epoch 14 Batch 500 Loss 0.2525\n","Epoch 14 Batch 550 Loss 0.1904\n","Epoch 14 Batch 600 Loss 0.2748\n","Epoch 14 Batch 650 Loss 0.2060\n","Epoch 14 Batch 700 Loss 0.2723\n","Epoch 14 Batch 750 Loss 0.2085\n","Epoch 14 Batch 800 Loss 0.1244\n","Epoch 14 Batch 850 Loss 0.2605\n","Epoch 14 Batch 900 Loss 0.2196\n","Epoch 14 Batch 950 Loss 0.2175\n","Epoch 14 Batch 1000 Loss 0.1701\n","Epoch 14 Batch 1050 Loss 0.2297\n","Epoch 14 Batch 1100 Loss 0.1779\n","Epoch 14 Batch 1150 Loss 0.1874\n","Epoch 14 Batch 1200 Loss 0.1897\n","Epoch 14 Batch 1250 Loss 0.1732\n","Epoch 14 Batch 1300 Loss 0.2044\n","Epoch 14 Batch 1350 Loss 0.1900\n","Epoch 14 Batch 1400 Loss 0.2056\n","Epoch 14 Batch 1450 Loss 0.1395\n","Epoch 14 Batch 1500 Loss 0.2521\n","Epoch 14 Batch 1550 Loss 0.1928\n","Epoch 14 Batch 1600 Loss 0.2637\n","Epoch 14 Batch 1650 Loss 0.1698\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 70%|███████   | 14/20 [3:17:43<1:22:36, 826.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 14 Loss 0.2045\n","Time taken for 1 epoch 803.3602607250214 sec\n","\n","Input: i am hungry.\n","Predicted translation: dagregre.\n","--------------------\n","Epoch 15 Batch 0 Loss 0.2158\n","Epoch 15 Batch 50 Loss 0.1400\n","Epoch 15 Batch 100 Loss 0.1861\n","Epoch 15 Batch 150 Loss 0.1685\n","Epoch 15 Batch 200 Loss 0.1037\n","Epoch 15 Batch 250 Loss 0.1702\n","Epoch 15 Batch 300 Loss 0.1907\n","Epoch 15 Batch 350 Loss 0.2201\n","Epoch 15 Batch 400 Loss 0.1655\n","Epoch 15 Batch 450 Loss 0.2561\n","Epoch 15 Batch 500 Loss 0.1761\n","Epoch 15 Batch 550 Loss 0.2139\n","Epoch 15 Batch 600 Loss 0.1494\n","Epoch 15 Batch 650 Loss 0.1480\n","Epoch 15 Batch 700 Loss 0.1985\n","Epoch 15 Batch 750 Loss 0.2064\n","Epoch 15 Batch 800 Loss 0.2015\n","Epoch 15 Batch 850 Loss 0.2265\n","Epoch 15 Batch 900 Loss 0.1730\n","Epoch 15 Batch 950 Loss 0.1514\n","Epoch 15 Batch 1000 Loss 0.1444\n","Epoch 15 Batch 1050 Loss 0.1851\n","Epoch 15 Batch 1100 Loss 0.1219\n","Epoch 15 Batch 1150 Loss 0.1859\n","Epoch 15 Batch 1200 Loss 0.1792\n","Epoch 15 Batch 1250 Loss 0.1553\n","Epoch 15 Batch 1300 Loss 0.1571\n","Epoch 15 Batch 1350 Loss 0.1727\n","Epoch 15 Batch 1400 Loss 0.1327\n","Epoch 15 Batch 1450 Loss 0.1634\n","Epoch 15 Batch 1500 Loss 0.1762\n","Epoch 15 Batch 1550 Loss 0.1666\n","Epoch 15 Batch 1600 Loss 0.1120\n","Epoch 15 Batch 1650 Loss 0.1716\n"],"name":"stdout"},{"output_type":"stream","text":["\r 75%|███████▌  | 15/20 [3:31:02<1:08:10, 818.05s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 15 Loss 0.1720\n","Time taken for 1 epoch 799.1972050666809 sec\n","\n","Input: i am hungry.\n","Predicted translation: logre.\n","--------------------\n","Epoch 16 Batch 0 Loss 0.2016\n","Epoch 16 Batch 50 Loss 0.1295\n","Epoch 16 Batch 100 Loss 0.1808\n","Epoch 16 Batch 150 Loss 0.1521\n","Epoch 16 Batch 200 Loss 0.1078\n","Epoch 16 Batch 250 Loss 0.1144\n","Epoch 16 Batch 300 Loss 0.1245\n","Epoch 16 Batch 350 Loss 0.1530\n","Epoch 16 Batch 400 Loss 0.1378\n","Epoch 16 Batch 450 Loss 0.1152\n","Epoch 16 Batch 500 Loss 0.1613\n","Epoch 16 Batch 550 Loss 0.1440\n","Epoch 16 Batch 600 Loss 0.1357\n","Epoch 16 Batch 650 Loss 0.1303\n","Epoch 16 Batch 700 Loss 0.1514\n","Epoch 16 Batch 750 Loss 0.1119\n","Epoch 16 Batch 800 Loss 0.1644\n","Epoch 16 Batch 850 Loss 0.1283\n","Epoch 16 Batch 900 Loss 0.1132\n","Epoch 16 Batch 950 Loss 0.1544\n","Epoch 16 Batch 1000 Loss 0.1444\n","Epoch 16 Batch 1050 Loss 0.1570\n","Epoch 16 Batch 1100 Loss 0.1665\n","Epoch 16 Batch 1150 Loss 0.1392\n","Epoch 16 Batch 1200 Loss 0.1549\n","Epoch 16 Batch 1250 Loss 0.1478\n","Epoch 16 Batch 1300 Loss 0.1218\n","Epoch 16 Batch 1350 Loss 0.1139\n","Epoch 16 Batch 1400 Loss 0.1689\n","Epoch 16 Batch 1450 Loss 0.1705\n","Epoch 16 Batch 1500 Loss 0.1763\n","Epoch 16 Batch 1550 Loss 0.1345\n","Epoch 16 Batch 1600 Loss 0.1116\n","Epoch 16 Batch 1650 Loss 0.1248\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 80%|████████  | 16/20 [3:44:16<54:03, 810.93s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Epoch 16 Loss 0.1466\n","Time taken for 1 epoch 794.3034212589264 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 17 Batch 0 Loss 0.1383\n","Epoch 17 Batch 50 Loss 0.1082\n","Epoch 17 Batch 100 Loss 0.1340\n","Epoch 17 Batch 150 Loss 0.1097\n","Epoch 17 Batch 200 Loss 0.1068\n","Epoch 17 Batch 250 Loss 0.1483\n","Epoch 17 Batch 300 Loss 0.1252\n","Epoch 17 Batch 350 Loss 0.1174\n","Epoch 17 Batch 400 Loss 0.1370\n","Epoch 17 Batch 450 Loss 0.1229\n","Epoch 17 Batch 500 Loss 0.1168\n","Epoch 17 Batch 550 Loss 0.1631\n","Epoch 17 Batch 600 Loss 0.1550\n","Epoch 17 Batch 650 Loss 0.0917\n","Epoch 17 Batch 700 Loss 0.1167\n","Epoch 17 Batch 750 Loss 0.0956\n","Epoch 17 Batch 800 Loss 0.1280\n","Epoch 17 Batch 850 Loss 0.1354\n","Epoch 17 Batch 900 Loss 0.1309\n","Epoch 17 Batch 950 Loss 0.1588\n","Epoch 17 Batch 1000 Loss 0.1436\n","Epoch 17 Batch 1050 Loss 0.1394\n","Epoch 17 Batch 1100 Loss 0.1197\n","Epoch 17 Batch 1150 Loss 0.1355\n","Epoch 17 Batch 1200 Loss 0.1008\n","Epoch 17 Batch 1250 Loss 0.1808\n","Epoch 17 Batch 1300 Loss 0.1604\n","Epoch 17 Batch 1350 Loss 0.0919\n","Epoch 17 Batch 1400 Loss 0.1199\n","Epoch 17 Batch 1450 Loss 0.1185\n","Epoch 17 Batch 1500 Loss 0.1358\n","Epoch 17 Batch 1550 Loss 0.1298\n","Epoch 17 Batch 1600 Loss 0.1437\n","Epoch 17 Batch 1650 Loss 0.1299\n","Epoch 17 Loss 0.1269\n","Time taken for 1 epoch 811.8011853694916 sec\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r 85%|████████▌ | 17/20 [3:57:48<40:33, 811.29s/it]"],"name":"stderr"},{"output_type":"stream","text":["Input: i am hungry.\n","Predicted translation: logregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregregre\n","--------------------\n","Epoch 18 Batch 0 Loss 0.1512\n","Epoch 18 Batch 50 Loss 0.1313\n","Epoch 18 Batch 100 Loss 0.1297\n","Epoch 18 Batch 150 Loss 0.1215\n","Epoch 18 Batch 200 Loss 0.1207\n","Epoch 18 Batch 250 Loss 0.1087\n","Epoch 18 Batch 300 Loss 0.1267\n","Epoch 18 Batch 350 Loss 0.1000\n","Epoch 18 Batch 400 Loss 0.1454\n","Epoch 18 Batch 450 Loss 0.0873\n","Epoch 18 Batch 500 Loss 0.1164\n","Epoch 18 Batch 550 Loss 0.1074\n","Epoch 18 Batch 600 Loss 0.1950\n","Epoch 18 Batch 650 Loss 0.0750\n","Epoch 18 Batch 700 Loss 0.1286\n","Epoch 18 Batch 750 Loss 0.0935\n","Epoch 18 Batch 800 Loss 0.1108\n","Epoch 18 Batch 850 Loss 0.1164\n","Epoch 18 Batch 900 Loss 0.1265\n","Epoch 18 Batch 950 Loss 0.1013\n","Epoch 18 Batch 1000 Loss 0.0748\n","Epoch 18 Batch 1050 Loss 0.1110\n","Epoch 18 Batch 1100 Loss 0.1090\n","Epoch 18 Batch 1150 Loss 0.0972\n","Epoch 18 Batch 1200 Loss 0.1287\n","Epoch 18 Batch 1250 Loss 0.0966\n","Epoch 18 Batch 1300 Loss 0.1291\n","Epoch 18 Batch 1350 Loss 0.1035\n","Epoch 18 Batch 1400 Loss 0.0997\n","Epoch 18 Batch 1450 Loss 0.1286\n","Epoch 18 Batch 1500 Loss 0.1389\n","Epoch 18 Batch 1550 Loss 0.0688\n","Epoch 18 Batch 1600 Loss 0.0951\n","Epoch 18 Batch 1650 Loss 0.1012\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["\r 90%|█████████ | 18/20 [4:11:56<27:24, 822.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 18 Loss 0.1107\n","Time taken for 1 epoch 847.516818523407 sec\n","\n","Input: i am hungry.\n","Predicted translation: tengo hambre.\n","--------------------\n","Epoch 19 Batch 0 Loss 0.1352\n","Epoch 19 Batch 50 Loss 0.1078\n","Epoch 19 Batch 100 Loss 0.1035\n","Epoch 19 Batch 150 Loss 0.0951\n","Epoch 19 Batch 200 Loss 0.0724\n","Epoch 19 Batch 250 Loss 0.1272\n","Epoch 19 Batch 300 Loss 0.0781\n","Epoch 19 Batch 350 Loss 0.1433\n","Epoch 19 Batch 400 Loss 0.1139\n","Epoch 19 Batch 450 Loss 0.0864\n","Epoch 19 Batch 500 Loss 0.1027\n","Epoch 19 Batch 550 Loss 0.1552\n","Epoch 19 Batch 600 Loss 0.1200\n","Epoch 19 Batch 650 Loss 0.1106\n","Epoch 19 Batch 700 Loss 0.0936\n","Epoch 19 Batch 750 Loss 0.0764\n","Epoch 19 Batch 800 Loss 0.0920\n","Epoch 19 Batch 850 Loss 0.0863\n","Epoch 19 Batch 900 Loss 0.0995\n","Epoch 19 Batch 950 Loss 0.0793\n","Epoch 19 Batch 1000 Loss 0.1064\n","Epoch 19 Batch 1050 Loss 0.1131\n","Epoch 19 Batch 1100 Loss 0.1038\n","Epoch 19 Batch 1150 Loss 0.0897\n","Epoch 19 Batch 1200 Loss 0.0882\n","Epoch 19 Batch 1250 Loss 0.0874\n","Epoch 19 Batch 1300 Loss 0.1307\n","Epoch 19 Batch 1350 Loss 0.0942\n","Epoch 19 Batch 1400 Loss 0.1047\n","Epoch 19 Batch 1450 Loss 0.1089\n","Epoch 19 Batch 1500 Loss 0.1211\n","Epoch 19 Batch 1550 Loss 0.0710\n","Epoch 19 Batch 1600 Loss 0.0939\n","Epoch 19 Batch 1650 Loss 0.0944\n"],"name":"stdout"},{"output_type":"stream","text":["\r 95%|█████████▌| 19/20 [4:26:06<13:50, 830.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 19 Loss 0.0989\n","Time taken for 1 epoch 850.1210560798645 sec\n","\n","Input: i am hungry.\n","Predicted translation: .\n","--------------------\n","Epoch 20 Batch 0 Loss 0.1147\n","Epoch 20 Batch 50 Loss 0.0991\n","Epoch 20 Batch 100 Loss 0.0888\n","Epoch 20 Batch 150 Loss 0.0850\n","Epoch 20 Batch 200 Loss 0.0705\n","Epoch 20 Batch 250 Loss 0.0746\n","Epoch 20 Batch 300 Loss 0.0770\n","Epoch 20 Batch 350 Loss 0.0837\n","Epoch 20 Batch 400 Loss 0.0844\n","Epoch 20 Batch 450 Loss 0.0619\n","Epoch 20 Batch 500 Loss 0.1016\n","Epoch 20 Batch 550 Loss 0.0594\n","Epoch 20 Batch 600 Loss 0.0917\n","Epoch 20 Batch 650 Loss 0.0511\n","Epoch 20 Batch 700 Loss 0.1312\n","Epoch 20 Batch 750 Loss 0.0746\n","Epoch 20 Batch 800 Loss 0.0874\n","Epoch 20 Batch 850 Loss 0.0983\n","Epoch 20 Batch 900 Loss 0.0762\n","Epoch 20 Batch 950 Loss 0.0840\n","Epoch 20 Batch 1000 Loss 0.0928\n","Epoch 20 Batch 1050 Loss 0.0844\n","Epoch 20 Batch 1100 Loss 0.0515\n","Epoch 20 Batch 1150 Loss 0.0743\n","Epoch 20 Batch 1200 Loss 0.0866\n","Epoch 20 Batch 1250 Loss 0.0615\n","Epoch 20 Batch 1300 Loss 0.1090\n","Epoch 20 Batch 1350 Loss 0.0790\n","Epoch 20 Batch 1400 Loss 0.0615\n","Epoch 20 Batch 1450 Loss 0.0915\n","Epoch 20 Batch 1600 Loss 0.0483\n","Epoch 20 Batch 1650 Loss 0.0967\n","Save Model!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 20/20 [4:39:45<00:00, 839.29s/it]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 20 Loss 0.0897\n","Time taken for 1 epoch 819.3941195011139 sec\n","\n","Input: i am hungry.\n","Predicted translation: .\n","--------------------\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"pPJWs8nsV_ac"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"y8yUoIGjQEtx"},"source":["### 載入模型\n","\n","這邊載入模型的方式是先建立模型，再把參數載入模型"]},{"cell_type":"code","metadata":{"id":"VqYSYhgLP--z"},"source":["# score_function = 'concat'\n","model_name = 'checkpoints_seq2seq_with_Bah_attention'\n","\n","# 建立模型\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n","decoder_Bahdanau = Decoder_Bahdanau(vocab_tar_size, embedding_dim, units)\n","# decoder_concat = Decoder_other(vocab_tar_size, embedding_dim, units, score_function)\n","\n","# 模型資訊\n","checkpoint_path = os.path.join(output_dir, model_name)\n","ckpt = tf.train.Checkpoint(encoder = encoder, decoder = decoder_Bahdanau, optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nN0Ql9zNlSJh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604945888449,"user_tz":-480,"elapsed":853,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"2a9c5d58-dff4-4cc9-b1c8-0a8546b3f097"},"source":["# 查看在資料夾底下有幾個模型\n","ckpt_manager.checkpoints"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['nmt_seq2seq_with_attention/checkpoints_seq2seq_with_Bah_attention/ckpt-8',\n"," 'nmt_seq2seq_with_attention/checkpoints_seq2seq_with_Bah_attention/ckpt-9',\n"," 'nmt_seq2seq_with_attention/checkpoints_seq2seq_with_Bah_attention/ckpt-10']"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"rOZi1gqHlT_q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604945896144,"user_tz":-480,"elapsed":2452,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"8bb02c1f-4d44-4e04-ecd9-4ae1881bffba"},"source":["# 選擇載入哪個模型\n","ckpt.restore(ckpt_manager.checkpoints[2])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f36a96c8160>"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"uqIjtqQgksZO"},"source":["### 注意力矩陣 (Attention map)\n","\n","西文翻譯網址: https://context.reverso.net/translation/spanish-english/estados+unidos+es+un+pais+mas+bonito"]},{"cell_type":"code","metadata":{"id":"5uLoZVYP6Wwk"},"source":["def plot_attention(attention, sentence, predicted_sentence):\n","  from matplotlib import ticker\n","  fig = plt.figure(figsize=(10,10))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention, cmap='viridis')\n","\n","  ax.set_xticklabels([''] + sentence, fontsize=14, rotation=90)\n","  ax.set_yticklabels([''] + predicted_sentence, fontsize=14)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nR3A8vLJmps"},"source":["def translate(sentence, decoder):\n","  pred, sentence, attention_plot = evaluate(sentence, decoder)\n","\n","  print('Input:\\n %s' % (sentence))\n","  print('Predicted translation:\\n %s' % (pred))\n","\n","  sentence = ['<BOS>'] + [tokenizer_en.decode([t]) for t in tokenizer_en.encode(sentence)] + ['<EOS>']\n","  pred = ['<BOS>'] + [tokenizer_sp.decode([t]) for t in tokenizer_sp.encode(pred)]\n","  \n","  plot_attention(attention_plot, sentence, pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLwdytxKJ-Iy","colab":{"base_uri":"https://localhost:8080/","height":725},"executionInfo":{"status":"ok","timestamp":1604947313898,"user_tz":-480,"elapsed":1071,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"6f6280d5-86c3-474a-9a87-bf54ca7d149c"},"source":["inp_sentence = 'America is a beautiful country.'\n","translate(inp_sentence, decoder_Bahdanau)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\n"," america is a beautiful country.\n","Predicted translation:\n"," estados unidos es un bello pais.\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"C2gzRXX6MjkI"},"source":[""],"execution_count":null,"outputs":[]}]}