{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"NLP_finetune.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJokIQ9NrWJE","executionInfo":{"status":"ok","timestamp":1606282751230,"user_tz":-480,"elapsed":752,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"6c331276-76d6-46da-efd5-e1bb7b5ad125"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Pm7Of_EgKEJ","executionInfo":{"status":"ok","timestamp":1606282753165,"user_tz":-480,"elapsed":1399,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"0e99ab82-7814-4b6f-88a9-4400b825ca0a"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Wed Nov 25 05:39:12 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t7XXZ02oafUF"},"source":["## Transformer Pre-trained model\n","\n","這一章節介紹目前自然語言處理最強大的模型- `Transformer`，`Transformer` 相較於 `RNN` 系列的模型，`Transformer` 在表現 (`metrics`) 以及計算效率 (`parallel`) 都有絕對的優勢，著名的 `pre-train` 模型如下，連結為各個模型的論文路徑，基本上這些模型都是 `Transformer` 的變形，不同的地方在於預訓練的策略，例如資料量大小、`Masked` 的差異以及 `Self-attention` 矩陣的差異，最特別的是最後一個 `ELECTRA`，是在`2019`年`11` 月初提出的論文，結合了 `transformer` 還有 `GAN`。\n","\n","* BERT: https://arxiv.org/abs/1810.04805\n"," - Masked Language Modeling + Next Sentence Prediction\n","\n","\n","* GPT: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n"," - AutoRegressive Prediction\n","\n","\n","* Transformer-XL: https://arxiv.org/abs/1901.02860\n"," - Learning dependency beyond a fixed length(>512)\n","\n","\n","* XLNet: https://arxiv.org/abs/1906.08237\n"," - Permutation Modeling\n","\n","\n","* XLM: https://arxiv.org/abs/1901.07291\n"," - Pretrain on cross-lingual language\n","\n","\n","* RoBERTa: https://arxiv.org/abs/1907.11692\n"," - Pretrain model longer, more data\n","\n","\n","* DistilBERT: https://arxiv.org/abs/1910.01108\n","* CTRL: https://arxiv.org/abs/1909.05858\n","* ELECTRA: https://openreview.net/pdf?id=r1xMH1BtvB\n"," - Transformer + GAN\n"," \n"," \n","### [GLUE Benchmark](https://gluebenchmark.com/leaderboard)"]},{"cell_type":"markdown","metadata":{"id":"Pkk0sJ2qafUF"},"source":["## [Transformer](https://huggingface.co/transformers/)\n","\n","這邊我們使用 `Transformers` 套件來進行 `finetune`，在進行 `finetune` 之前，需要了解自然語言處理任務上的差異，最主要分為兩種分類任務：\n","\n","1. `Text classification`: 輸入一個句子，輸出該句子的分類。\n","2. `Sentence-Pair classification`: 輸入兩個句子的pair，輸出兩個句子之間的關係。\n","\n","* PS. 這些預訓練模型除了表現亮眼之外，最重要的貢獻在於預訓練後的 `word embedding`，`word embedding` 表示在文本中，詞與詞之間的關係，最著名的例子就是: 男性 - 女性 = 國王 - 皇后，像這樣的對應關係，有訓練良好的 `word embedding` ，基本上在其他應用任務表現也會不錯，例如聊天機器人、推薦系統等等。\n","\n","在這裡我們會使用 `BERT` 來進行 `finetune`。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4sMCtx-SafUF","executionInfo":{"status":"ok","timestamp":1606282767535,"user_tz":-480,"elapsed":10670,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"f1e5a812-0c80-491b-fe46-cfaa9aa1ab3f"},"source":["!pip install transformers\n","!pip install sacremoses"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 5.6MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 20.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 43.1MB/s \n","\u001b[?25hCollecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 47.0MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a032bf976fd3ca908c80f5578c624f9db18e2840cafa0c1b7820aac96ef72203\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.17.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yFvc0RDYafUI"},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","from transformers import TFBertForSequenceClassification, BertTokenizer, glue_convert_examples_to_features, TFXLNetForSequenceClassification, XLNetTokenizer\n","\n","os.chdir('/content/drive/Shareddrives/類技術班教材/標準版/NLP進階/Seq2seq 系列模型/4.Transformer_based_model/Finetune_on_glue')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oupmx_DIafUI"},"source":["## 模型名稱解釋\n","\n","* `bert-base-uncased`:\n","  - `bert`: 模型名稱\n","  - `base`: 模型大小，`base` 表示層數為$12$層, `word embedding(hidden)` 為$768$維, `heads` 為$12$，另外有 `large`，層數為$24$層，`word embedding(hidden)` 為$1024$維，`heads` 為$16$。\n","  - `uncased`: 表示對於文本的前處理，`uncased` 表示字全部轉小寫，反之 `cased` 表示維持原樣。\n"," \n","另外不只有這些模型，其餘模型可以參考：\n","https://huggingface.co/transformers/pretrained_models.html"]},{"cell_type":"code","metadata":{"id":"RqoHIn09wiJA"},"source":["# model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n","# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zNL2qWtafUI","executionInfo":{"status":"ok","timestamp":1605777880407,"user_tz":-480,"elapsed":5041,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"60c55556-c42b-4f11-967a-d74980468823"},"source":["\"\"\"\n","載入預訓練模型\n","\"\"\"\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_75', 'classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"yrXyZctVafUI"},"source":["\"\"\"\n","載入模型斷詞工具\n","\"\"\"\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBxNREnNafUI"},"source":["## Finetune\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?export=view&id=1uOR-Xmf0Gd8fXuaFIW2oew-OUkCAt4b_' width=\"800\"/>\n","<figcaption>Self-attention</figcaption></center>\n","</figure>\n","\n","所有預訓練模型都是在 [GLUE Benchmark](https://gluebenchmark.com/leaderboard) 進行競賽，這個競賽提供多種不同的自然語言處理任務，這些任務都是屬於分類任務，只是差別在於資料集大小以及來源而已，這裡我們使用其中一種分類任務 `MRPC` 來進行 `finetune`。\n","\n","* 資料來源: [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview#wmt19_translate)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtzAplseafUI","executionInfo":{"status":"ok","timestamp":1605777883669,"user_tz":-480,"elapsed":1562,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"c640d41d-4106-42e0-cde0-efed534fa36d"},"source":["data, info = tfds.load('glue/mrpc', with_info=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:absl:Load dataset info from /root/tensorflow_datasets/glue/mrpc/1.0.0\n","INFO:absl:Reusing dataset glue (/root/tensorflow_datasets/glue/mrpc/1.0.0)\n","INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/glue/mrpc/1.0.0\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"F-9L16p-afUI"},"source":["### Info\n","\n","資料集的介紹，最需要注意的地方就是資料集的樣子，因為 `MRPC` 是屬於 `Sentnece-Pair classification` 任務，所以資料集包括了 `sentence1` 和 `sentence2` 對應一個 `label`，`MRPC` 主要是在分類兩個句子之間的語義是否相同，`label` 為$1$表示相同，反之$0$表示不同。\n","\n","因為是競賽資料集，所以資料集已經切割好為 `train`, `validation` 以及 `test`。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4xUt8z0afUI","executionInfo":{"status":"ok","timestamp":1605777884961,"user_tz":-480,"elapsed":2517,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"81079aa4-d109-44a0-f6da-6e9da0c409a5"},"source":["info"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tfds.core.DatasetInfo(\n","    name='glue',\n","    version=1.0.0,\n","    description='GLUE, the General Language Understanding Evaluation benchmark\n","(https://gluebenchmark.com/) is a collection of resources for training,\n","evaluating, and analyzing natural language understanding systems.',\n","    homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398',\n","    features=FeaturesDict({\n","        'idx': tf.int32,\n","        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n","        'sentence1': Text(shape=(), dtype=tf.string),\n","        'sentence2': Text(shape=(), dtype=tf.string),\n","    }),\n","    total_num_examples=5801,\n","    splits={\n","        'test': 1725,\n","        'train': 3668,\n","        'validation': 408,\n","    },\n","    supervised_keys=None,\n","    citation=\"\"\"@inproceedings{dolan2005automatically,\n","      title={Automatically constructing a corpus of sentential paraphrases},\n","      author={Dolan, William B and Brockett, Chris},\n","      booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\n","      year={2005}\n","    }\n","    @inproceedings{wang2019glue,\n","      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n","      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n","      note={In the Proceedings of ICLR.},\n","      year={2019}\n","    }\n","    \n","    Note that each GLUE dataset has its own citation. Please see the source to see\n","    the correct citation for each contained dataset.\"\"\",\n","    redistribution_info=,\n",")"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-d499jFafUI","executionInfo":{"status":"ok","timestamp":1605777884963,"user_tz":-480,"elapsed":2343,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"b3111842-c5a6-408e-c6f5-a1b2869703ae"},"source":["for k, v in data.items():\n","    print('key:', k)\n","    print('data shapes:\\n', v)\n","    print('-' * 20)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["key: test\n","data shapes:\n"," <PrefetchDataset shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>\n","--------------------\n","key: train\n","data shapes:\n"," <PrefetchDataset shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>\n","--------------------\n","key: validation\n","data shapes:\n"," <PrefetchDataset shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>\n","--------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kK1F-o39afUJ"},"source":["### Dataset overview\n","\n","`tensorflow` 儲存資料的方式都是以 `tf.data.Data` 型態來儲存，可以使用 `iter` 來建立 `generator`，並使用 `next` 來觀看第一筆資料，資料中包含了 `idx`、`label`、`sentence1` 以及 `sentence2`。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yz--AQfUafUJ","executionInfo":{"status":"ok","timestamp":1605777884963,"user_tz":-480,"elapsed":1881,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"28fa442a-fa2d-4536-b789-8503fc7e7e9b"},"source":["assert isinstance(data['train'], tf.data.Dataset)\n","\n","temp = data['train']\n","temp_gen = iter(temp)\n","next(temp_gen)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1680>,\n"," 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>,\n"," 'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'The identical rovers will act as robotic geologists , searching for evidence of past water .'>,\n"," 'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The rovers act as robotic geologists , moving on six wheels .'>}"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"hjjVl88HafUJ"},"source":["### Training data format"]},{"cell_type":"markdown","metadata":{"id":"3tT8xss9afUJ"},"source":["接下來我們需要將資料集轉換成模型可讀取的格式，輸入格式有三個：\n","\n","* `input_ids`: 這表示句子斷完詞之後轉成 `token embeddings`，每一個詞有一個 `id`，如下圖，其中 `101` 表示 `[CLS]`，`102` 表示 `[SEP]`，因為 `MPRC` 是 `Sentence-Pair classification` 任務，所以下面的範例中會看到兩個 `102`。\n","\n","* `attention mask`: 因為 `Transformer` 會限制輸入句子的長度，最大限制為 `512`，而我們選擇 `128`，但不是所有的句子長度都是 `128`，所以需要在後面進行 `padding` (就是補0)，最主要的目的是不去計算 `padding` 位置的 `loss` 。\n","\n","* `token_type_ids`: 用來表示 `Segment embedding`，如上圖，表示詞屬於哪一個句子，因為 `MRPC` 有兩個句子，所以 `ids` 有2種，`0` 和 `1`。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tIS-FiN1afUJ","executionInfo":{"status":"ok","timestamp":1605777891846,"user_tz":-480,"elapsed":8319,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"fac8d577-c0c0-4623-dd41-d4dbdf5426bf"},"source":["max_length = 128\n","task = 'mrpc'\n","\n","train_dataset = glue_convert_examples_to_features(data['train'],\n","                                                  tokenizer,\n","                                                  max_length,\n","                                                  task)\n","valid_dataset = glue_convert_examples_to_features(data['validation'],\n","                                                  tokenizer,\n","                                                  max_length,\n","                                                  task)\n","test_dataset = glue_convert_examples_to_features(data['test'],\n","                                                 tokenizer,\n","                                                 max_length,\n","                                                 task)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n","  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n","  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"6HSL-e1vafUJ"},"source":["### Example\n","\n","觀察轉換過後的資料集。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cP-oG9nafUJ","executionInfo":{"status":"ok","timestamp":1605777891848,"user_tz":-480,"elapsed":7184,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"b0a2ca16-ff27-421e-e190-b5d73899b045"},"source":["next(iter(train_dataset))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'attention_mask': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n","  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n","  'input_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n","  array([  101,  1996,  7235,  9819,  2097,  2552,  2004, 20478, 21334,\n","          2015,  1010,  6575,  2005,  3350,  1997,  2627,  2300,  1012,\n","           102,  1996,  9819,  2552,  2004, 20478, 21334,  2015,  1010,\n","          3048,  2006,  2416,  7787,  1012,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0], dtype=int32)>,\n","  'token_type_ids': <tf.Tensor: shape=(128,), dtype=int32, numpy=\n","  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>},\n"," <tf.Tensor: shape=(), dtype=int64, numpy=0>)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"WWsXusTfafUJ"},"source":["### Parameter settings\n","\n","在 `tf.data.Dataset` 中，通常會在訓練資料集後面接上三個標準的操作：\n","\n","* `.shuffle()`: 打亂資料集的方式，會先從資料集中隨機抽取`buffer_size`筆資料進去 `buffer`，然後再 `buffer` 從中抽取`batch_size`筆資料進行訓練，丟進 `buffer` 的步驟主要是在處理無法一次將所有資料集丟進記憶體進行訓練的情形。\n","\n","* `.batch()`: 每次迭代使用的資料數量。\n","* `.repeat()`: `epochs` 數量。"]},{"cell_type":"code","metadata":{"id":"ipXz0uBlafUJ"},"source":["buffer_size = 100\n","train_bz = 16\n","epochs = 3\n","valid_bz = 50\n","\n","train_dataset = train_dataset.shuffle(buffer_size).batch(train_bz).repeat(epochs)\n","valid_dataset = valid_dataset.batch(valid_bz)\n","test_dataset = test_dataset.batch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vcV_NV6RafUJ"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5,\n","                                     epsilon=1e-8,\n","                                     clipnorm=1.0)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","                                                     reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDvm8fFUafUJ"},"source":["## Training\n","\n","* `.fit()`: 支援 `generator` 的輸入方式，也可以用 `fit_generator` 。\n","* `steps_per_epoch`: 每個 `epoch` 訓練幾次，通常是 $\\frac{train\\_size}{batch\\_size}$ ，遍歷整個訓練集。\n","* `validation_steps`: 與 `steps_per_epoch` 同義。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lznRoAqoafUJ","executionInfo":{"status":"ok","timestamp":1605778046694,"user_tz":-480,"elapsed":160539,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"0c20ece1-42ce-4433-a60b-3a91f4747062"},"source":["history = model.fit(train_dataset,\n","                    epochs=epochs,\n","                    steps_per_epoch=3668//train_bz, \n","                    validation_data=valid_dataset,\n","                    validation_steps=408//valid_bz)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","229/229 [==============================] - 47s 205ms/step - loss: 0.6114 - accuracy: 0.6692 - val_loss: 0.5167 - val_accuracy: 0.7500\n","Epoch 2/3\n","229/229 [==============================] - 44s 191ms/step - loss: 0.4581 - accuracy: 0.7859 - val_loss: 0.4852 - val_accuracy: 0.8075\n","Epoch 3/3\n","229/229 [==============================] - 43s 190ms/step - loss: 0.2948 - accuracy: 0.8801 - val_loss: 0.5585 - val_accuracy: 0.8150\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6JJQt8E0afUJ"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"21xZWMaqafUJ"},"source":["valid_pred = model.predict(valid_dataset)\n","valid_pred_ids = np.argmax(valid_pred[0], axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWDChDSbafUJ"},"source":["import numpy as np\n","\"\"\"\n","從 tf.data.Dataset 中拿取 label\n","\"\"\"\n","valid_label = list()\n","for x in valid_dataset:\n","  valid_label.append(x[1].numpy())\n","valid_label = np.concatenate(valid_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"_VaR4FkpafUJ","executionInfo":{"status":"ok","timestamp":1605778104799,"user_tz":-480,"elapsed":1040,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"f3cc80bf-7056-4241-98e1-97b39bb4c7cc"},"source":["confm = confusion_matrix(y_pred=valid_pred_ids, y_true=valid_label)\n","\n","index = ['Actual_0', 'Actual_1']\n","columns = ['Pred_0', 'Pred_1']\n","pd.DataFrame(confm, index=index, columns=columns)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pred_0</th>\n","      <th>Pred_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Actual_0</th>\n","      <td>61</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>Actual_1</th>\n","      <td>7</td>\n","      <td>272</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          Pred_0  Pred_1\n","Actual_0      61      68\n","Actual_1       7     272"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzbXNvq10JcM","executionInfo":{"status":"ok","timestamp":1605778104799,"user_tz":-480,"elapsed":881,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"f399daef-78ff-4b05-cb54-505d3015c0cc"},"source":["print(classification_report(y_pred=valid_pred_ids, y_true=valid_label))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.90      0.47      0.62       129\n","           1       0.80      0.97      0.88       279\n","\n","    accuracy                           0.82       408\n","   macro avg       0.85      0.72      0.75       408\n","weighted avg       0.83      0.82      0.80       408\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rkgSzVLXafUJ"},"source":["## Save model"]},{"cell_type":"code","metadata":{"id":"aBL0XIlUafUJ"},"source":["save_path = 'save'\n","if not os.path.exists(save_path):\n","    os.mkdir(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFxC_0ilafUJ"},"source":["model.save_pretrained(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vgEhNQyjafUJ"},"source":["## Load model and predict\n","\n","這邊參考`MRPC`的輸入格式，一樣會使用`glue_convert_examples_to_features`這個函數進行轉換。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FHR82H6afUJ","executionInfo":{"status":"ok","timestamp":1605778117909,"user_tz":-480,"elapsed":5510,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"e476fb51-cd55-496b-b9ac-e8bbca43c76a"},"source":["new_model = TFBertForSequenceClassification.from_pretrained(save_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at save were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at save and are newly initialized: ['dropout_113']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lIV9EB59afUJ"},"source":["sentence1 = [\"Anorld Schwarzenegger is my idol.\"]\n","sentence2 = [\"My favorite idol is Anorld Schwarzenegger.\"]\n","\n","test_dataset = pd.DataFrame(dict(idx=list(range(len(sentence1))),\n","                                 label=[0]*len(sentence1),\n","                                 sentence1=sentence1,\n","                                 sentence2=sentence2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"UDzKnZUJafUJ","executionInfo":{"status":"ok","timestamp":1605778117911,"user_tz":-480,"elapsed":5012,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"775c54c0-c4d5-48fc-8e78-ba00193c07e6"},"source":["\"\"\"\n","模仿GLUE的輸入格式: (idx, label, sentence1, sentence2)\n","其中label是假的，是因為輸入需要，不會影響預測值\n","\"\"\"\n","test_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idx</th>\n","      <th>label</th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Anorld Schwarzenegger is my idol.</td>\n","      <td>My favorite idol is Anorld Schwarzenegger.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   idx  ...                                   sentence2\n","0    0  ...  My favorite idol is Anorld Schwarzenegger.\n","\n","[1 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"1cNK35eoafUJ"},"source":["test_gen = tf.data.Dataset.from_tensor_slices(dict(test_dataset))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pK8Ge_-4afUJ","executionInfo":{"status":"ok","timestamp":1605778117912,"user_tz":-480,"elapsed":4697,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"8a458acd-289f-4eb9-d719-d0c2001ffb61"},"source":["test_gen = glue_convert_examples_to_features(test_gen, tokenizer, max_length, task)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n","  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n","  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"C6J9-wlXafUJ"},"source":["test_gen = test_gen.batch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Ue1Ny9fafUJ","executionInfo":{"status":"ok","timestamp":1605778117913,"user_tz":-480,"elapsed":4165,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"9c698436-e68d-4cb9-916d-3f511977e1b5"},"source":["next(iter(test_gen))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n","  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","        dtype=int32)>,\n","  'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n","  array([[  101,  2019,  2953,  6392, 29058,  8625, 13327,  2003,  2026,\n","          10282,  1012,   102,  2026,  5440, 10282,  2003,  2019,  2953,\n","           6392, 29058,  8625, 13327,  1012,   102,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]], dtype=int32)>,\n","  'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n","  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","        dtype=int32)>},\n"," <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"j9cFFHjCafUJ"},"source":["pred = new_model.predict(test_gen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dd7BBbAsafUJ"},"source":["pred_ids = np.argmax(pred, axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hA3UWfQeafUJ","executionInfo":{"status":"ok","timestamp":1605778120608,"user_tz":-480,"elapsed":5110,"user":{"displayName":"康文瑋","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0kw0qBY7pljC_Bn7wCPKwga44ziqqbm67ccpq=s64","userId":"14275451592269779869"}},"outputId":"c5f1f7d1-814c-4243-8def-320021882172"},"source":["print(pred_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1OtKXecQafUJ"},"source":[""],"execution_count":null,"outputs":[]}]}